1600346794369888257 2022-12-07 04:29:43 +0000 <YiMaTweets> Supervised:  https://t.co/a185jAkXLO Incremental:  https://t.co/Ptai0xJzsC and Unsupervised:  https://t.co/FvsLgJNSHt The current models still require global optimization, but I believe there are  mechanisms to learn such closed-loop models locally.
1600346288339329024 2022-12-07 04:27:42 +0000 <YiMaTweets> I am glad to see that Hinton recognizes that nature adopts loops for learning. Indeed, our latest work suggest closed-loop framework unifies learning in almost all settings: supervised, incremental, unsupervised, for both discriminative and generative purposes.
1600344813131341824 2022-12-07 04:21:51 +0000 <YiMaTweets> With a clear loss function, deep networks can be constructed or derived in a purly forward fashion, as a white box. See ReduNet:  https://t.co/CN7eurqo2w  and its ancestor PCANet:  https://t.co/3kkeN0EgfX
1600344122044604416 2022-12-07 04:19:06 +0000 <YiMaTweets>  https://t.co/h8Gi7B6pWd Just want remind: we have been advocating for years deep networks can be forward-constructed to optimize learned features since PCANet and ReduNet. BP is a way to fine-tune, but not the natural way. Closed-loop feedback is a more natural to correct errors.
1600301687180185600 2022-12-07 01:30:29 +0000 <YiMaTweets> @Saul_Cheung88 Yes!
1600301551699914752 2022-12-07 01:29:56 +0000 <YiMaTweets> One does not follow Boeing's biggest and newest airplanes to learn more about air dynamics, or about how birds fly.
1600278337133641728 2022-12-06 23:57:41 +0000 <YiMaTweets> Honestly, I was serious looking forward to seeing a soccer match between Spain and Portugal...
1600177835397681152 2022-12-06 17:18:20 +0000 <YiMaTweets> Essentially the closed loop transcription is in the same spirit: the encoder is lossy and the objective function (for generation or prediction) is entirely on the learned feature space. That is the only way learning can be internalized hence autonomous, as done in nature.
1599943182602887168 2022-12-06 01:45:54 +0000 <YiMaTweets> But this one gives the semantically wrong answer in the mathematically right way though. I am kind of impressed (as this might be a solution a kid might give) :-)
1599937777474629632 2022-12-06 01:24:26 +0000 <YiMaTweets> @AnimaAnandkumar That is precisely my viewpoint too: learning does not have be entirely task-specific and it could just learn what is reproducible and predictable (in the data).
1599903333216841729 2022-12-05 23:07:34 +0000 <YiMaTweets> I think it depends on the interpretation of "generative models". I believe here Yann meant not to regenerate every details of the data (distribution), instead to learn what is useful for prediction/action. There should be a selection process for what is worth learning.
1599898675605798912 2022-12-05 22:49:03 +0000 <YiMaTweets> @tng_konrad good one!
1599884882603560960 2022-12-05 21:54:15 +0000 <YiMaTweets> I will be visiting Urbana-Champaign next week and give a distinguished lecture at the CS Department:  https://t.co/NqhC2JWQR3 I actually planned a visit two years ago but cancelled due to the pandemic. It is great that I will be seeing many old friends there again.
1599606388074123272 2022-12-05 03:27:36 +0000 <YiMaTweets> My new book is often one of the best selling signal processing books on Amazon. My kids once asked me, why my book is listed next to books called "XXX for Complete Idiots"... I do not know how to explain.
1599570981441671168 2022-12-05 01:06:55 +0000 <YiMaTweets> Just received the money from the first royalty check of my new book on high-dim data analysis. Just in time to buy some nice Xmas gifts for my kids and wife, symbolically compensating for all the time that the book took away from them...
1599494259627134976 2022-12-04 20:02:03 +0000 <YiMaTweets> @DrElectronX well, change "a lie" to "enough lies".
1599305804221648901 2022-12-04 07:33:12 +0000 <YiMaTweets> I have been back at Berkeley for five years and so far have had three Phd students graduated under my supervision: one from Berkeley EECS and two from Tsinghua-Berkeley Shenzhen Institute. One in 3D vision, one in Optimization, and one in Machine Learning. Very rewarding...
1599303823675494400 2022-12-04 07:25:19 +0000 <YiMaTweets> My second PhD student co-supervised with Tsinghua-Berkeley Shenzhen Institute (TBSI), Mingyang Li, passed his defense today. Congratulations, and very proud of you!  https://t.co/7Ff1Ma3LHL
1599217656321933312 2022-12-04 01:42:55 +0000 <YiMaTweets> @ChaosdogeP True... I will be phasing out of Weibo. At least will minimize my contribution to its popularity...
1599216415869112320 2022-12-04 01:38:00 +0000 <YiMaTweets> @artistexyz I am pretty sure I will find another social network to use... or start writing books instead.
1599211631132954626 2022-12-04 01:18:59 +0000 <YiMaTweets> If your Weibo account has never been suspended, then you have never spoken the truth; if your Twitter account has never been suspended, then you probably have never told a lie.
1599115517671137280 2022-12-03 18:57:04 +0000 <YiMaTweets> Well, as for my suspended Weibo account, since I cannot post anything at all during this time, just think of it as a piece of empty white paper that says everything that there needs to be said.
1599109080194625538 2022-12-03 18:31:29 +0000 <YiMaTweets> My Chinese Weibo account is suspended for fifteen days, without giving any reason except that "some content violated certain law". Well, as far as I know, forbidding citizens to speak freely violates the constitution of China...
1598785528916512769 2022-12-02 21:05:48 +0000 <YiMaTweets> an insider joke...
1598424368375533570 2022-12-01 21:10:41 +0000 <YiMaTweets> Learning done by individuals within one life time (ontogenetic) is through feedback (BP or closed-loop), which needs to be efficient; learning by species through generations (phylogenetic) could be done through random selection or search, inefficient, hence needs to be passed on.
1597702072735977472 2022-11-29 21:20:32 +0000 <YiMaTweets> Start to see quite a few "theoretical" papers on analyzing diffusion models. Folks, a good theory is supposed to lead and project, not to always trail as  hindsight...
1597695353972617216 2022-11-29 20:53:50 +0000 <YiMaTweets> AI is better for "assisted intelligence" -- "intelligence" of the machine, if any at all, is transplanted with human assistance: from data preparation, to network/system design, to training/optimization, and to evaluation how good it is. Not even autonomous, let alone sentient...
1597689940472496128 2022-11-29 20:32:20 +0000 <YiMaTweets> Thanks to my fellow Chinese colleagues, my new book on high-dim data analysis:  https://t.co/4XEBJXNwqn has been translated into Chinese -- blazingly fast considering the book was out only 7 months ago. After proofreading and copyediting, it should come out early next year.
1597688465193586690 2022-11-29 20:26:28 +0000 <YiMaTweets> I believe the necessary and sufficient conditions are given by the so called Schoenberg Theorem. That is a classic example we give in my new book:  https://t.co/4XEBJY5FEv, Theorem 4.1 of Chapter 4.
1597031765608013826 2022-11-28 00:56:58 +0000 <YiMaTweets> @ericjang11 I think everyone sees different things from the paper: what is right and what is wrong, in their own conscience...
1597002691493830656 2022-11-27 23:01:27 +0000 <YiMaTweets> So glad to meet so many old and new friends at Hong Kong University! Also, gave a talk about our latest developments on Compressive Closed-Loop Transcription, with video recording available at:  https://t.co/TAG6W9Srri  https://t.co/RPh7BrrWBk
1596983077296738304 2022-11-27 21:43:30 +0000 <YiMaTweets> For all my colleagues who study information science or data science, do you know how rich information that one piece of *white paper* can contain; and how massive information that one sentence "you know what I want to say" can communicate? That beats all noises on the internet.
1596976357363548161 2022-11-27 21:16:48 +0000 <YiMaTweets> Probably everyone know what is happening in China these days. When I started my tweeter account, I decided to use it only for academia purposes, not to discuss anything political (unless my Chinese Weibo account was suspended.) But anyone with a conscience should pay attention.
1596974955014418433 2022-11-27 21:11:14 +0000 <YiMaTweets> Had a great trip to Hong Kong during the holidays‚Ä¶ pictures from the faculty apartment, from the hotel, and a restaurant‚Ä¶ Hong Kong is a truly 3D city. Like Chongqing city in China‚Ä¶ nightmare for Po though ‚Äî stairs everywhere‚Ä¶  https://t.co/7t93mRFd6u
1595298413838405632 2022-11-23 06:09:15 +0000 <YiMaTweets> For many ‚Äútexts to images‚Äù generation, l actually find Google images searched with texts are not so bad at all‚Ä¶
1595192179974029312 2022-11-22 23:07:07 +0000 <YiMaTweets> Cool...
1595157018356375552 2022-11-22 20:47:24 +0000 <YiMaTweets> Will talk about newest developments at HKU this Friday: showing a potential unified framework for all settings of learning (supervised, incremental, unsupervised, discriminative, generative, autoencoding), with the simplest, most efficient, and white-box networks/architectures.  https://t.co/pCan9mgdSY
1595149328821080064 2022-11-22 20:16:51 +0000 <YiMaTweets> I was just told that our paper was among top 10 in July... Not so bad for a theoretical position paper... Truly hope this opens up new avenues of study for intelligence in the future.
1594585976742981632 2022-11-21 06:58:17 +0000 <YiMaTweets> I start to wish that, after Elon Mush is done with Twitter, he could consider buying the Chinese soccer organization and team.
1594483067162238976 2022-11-21 00:09:21 +0000 <YiMaTweets> @PeteTanski Let's hope so...
1594424854035382272 2022-11-20 20:18:02 +0000 <YiMaTweets> I created my Twitter account earlier this year in case my Chinese Weibo account be suspended as it becomes increasingly politically sensitive. Well, if Twitter becomes  someone's business &amp; political PR platform in coming days, I will definitely have to find yet another platform.
1593810298791526401 2022-11-19 03:36:01 +0000 <YiMaTweets> I am actually teaching related stuff in my course this semester... only a little late but should be valuable to others...
1593806834304921602 2022-11-19 03:22:15 +0000 <YiMaTweets> @CSProfKGD Such papers do not bother me. The problem is that such papers often cause tremendous confusion among the students and we teachers have to do extra job to deal with the aftermath...
1593768990114926592 2022-11-19 00:51:52 +0000 <YiMaTweets> Actually the problem with many such papers is that obviously the authors do NOT know the main or correct reason to the better performance, hence just make some up which often turn out not to be the case. It is more of a confusion, instead of a contribution...
1593701692121354241 2022-11-18 20:24:27 +0000 <YiMaTweets> What a year! Finally, something positive to look forward to in the world, before the end of the year!  https://t.co/Hd5GMzsxdV
1593513573161918465 2022-11-18 07:56:56 +0000 <YiMaTweets> Our intuition about high-dim spaces is often limited and even wrong. The space of natural images in the high-dim space of all possible "images" is tiny... The denoising process is probably one effective way to access it, but definitely not the only way nor the best way...
1593512190052823040 2022-11-18 07:51:26 +0000 <YiMaTweets> When I worked on face recognition 15 years ago, I realized how much info can be hidden in a tiny volume in a high-dim space. Face ID is like "finding a needle in a haystack which itself lies on the tip of a needle," yet, miraculously doable! That led me to high-dim data analysis.
1593508038639374336 2022-11-18 07:34:57 +0000 <YiMaTweets> Probably it is time to revisit some of the old goodies:  https://t.co/CIO8d2hGSY that we did more than ten years ago? See  what kind of crazy applications folks can make out of with far more advanced methods than the simple denoising/diffusion?
1593496065944088576 2022-11-18 06:47:22 +0000 <YiMaTweets> Berkeley is on strike big time this week. Probably it will spread to Twitter now?
1593345419706216448 2022-11-17 20:48:45 +0000 <YiMaTweets> All technologies are double-edged swords. I learned that lesson with face recognition. When I was working on it, it was just a very intellectually challenging problem and we made some exciting discovery about high-dim data and statistics. Once the genie is out of the bottle...
1592993748967120897 2022-11-16 21:31:20 +0000 <YiMaTweets> To give a Keynote at the upcoming DeepLearn 2023 Winter in mid January, Bournemouth, UK:  https://t.co/L91d0UG2jI Funny thing is that now I can use the visa that I applied for the summer family vacation to London which I did not go (as the visa arrived two weeks after the trip).
1592384661564841984 2022-11-15 05:11:03 +0000 <YiMaTweets> I feel really sorry for today‚Äôs young researchers‚Ä¶ at least this was not why academia attracted me‚Ä¶
1591883127882121216 2022-11-13 19:58:08 +0000 <YiMaTweets> Simplest way to understand the roles of Parsimony and Self-Consistency in learning: Parsimony is about "compress to learn &amp; learn to compress"; but naive compression can lead to collapsing, hence Self-consistency is to make sure what is learned is faithful to what is observed.
1591881435971190784 2022-11-13 19:51:24 +0000 <YiMaTweets> "Symmetry" is equivalent to what being viewed as a "equivalent" set to compress. Compressing coding rate for trans.-inv. signals gives convolution operators. By definition, coding rate is invariant to permutation. Hence optimizing rate distortion leads to self-attention operators
1591865719104622592 2022-11-13 18:48:57 +0000 <YiMaTweets> Happy 50th birthday last night with students, colleagues, and friends. Awesome since 1972!  https://t.co/LH4iEFgAyK
1591568891465326593 2022-11-12 23:09:28 +0000 <YiMaTweets> ‚ÄúMost people overestimate what they can do in one year and underestimate what they can do in ten years.‚Äù ‚Äï Bill Gates. I am a less patient person hence five is more like it. In retrospect, it feels like I have done more than ten years of work in the past five years at Berkeley.
1591564611035320320 2022-11-12 22:52:27 +0000 <YiMaTweets> Of course, many related positions will also be available in the coming years, including researchers, engineers,  postdocs, and graduate students. Young researchers and scientists in related areas probably should pay more attention to this great place:  https://t.co/B9qZ24z8Ur
1591563707930546176 2022-11-12 22:48:52 +0000 <YiMaTweets> I always challenge myself every 5 years. Starting in Jan. 1, I will serve as the director of a new Institute of Data Science of Hong Kong University. The institute is very well funded and plans to recruit up to 30 new faculty in the general areas of data science and intelligence.
1591341838580871168 2022-11-12 08:07:14 +0000 <YiMaTweets> Instead of using blockchain for digital currency and burning billions, can we use it to design voting technology so that every cell phone can be a reliable voting booth? Certifiable and privacy preserving? Leaving no room for anyone to question voting fairness and accessibility?
1591210037653143552 2022-11-11 23:23:30 +0000 <YiMaTweets> Call me superstitious, but it seems to take some faith to invest your money in a financial firm whose CEO's last name is "Bankman-Fried"...
1591160274241073152 2022-11-11 20:05:46 +0000 <YiMaTweets> @TuriGuiliano4 Yes, indeed, the father of the Republic of China! The pioneer for a free China and rights for the people.
1591153331803934721 2022-11-11 19:38:11 +0000 <YiMaTweets> Will be exactly 50 years old tomorrow. But somehow got the feeling I just started to realize what my life is all about. Confucius says: "‰∫îÂçÅËÄåÁü•Â§©ÂëΩ„ÄÇ" (At age of fifty, knowing your destiny .) Just wish I had not waited this long though...
1589820236974657538 2022-11-08 03:20:56 +0000 <YiMaTweets> I have only used twitter for a little over half a year... and suddenly some folks around are talking about possible exits? Hope twitter remains a good platform for academic communities...
1589474382514356224 2022-11-07 04:26:38 +0000 <YiMaTweets> Why don't we see this on zoom meeting yet?
1589428738122317824 2022-11-07 01:25:15 +0000 <YiMaTweets> @JeppeJohansen3 jolliffe‚Äôs book on PCA‚Ä¶ but the full story is not in any book
1589428000311087104 2022-11-07 01:22:20 +0000 <YiMaTweets> @BlumLenore Yes!
1589357820771987456 2022-11-06 20:43:27 +0000 <YiMaTweets> I am teaching SVD to Berkeley first year students‚Ä¶ this is one of the very few magical things that have simultaneously elegant algebraic, geometric, and statistical interpretations and origins‚Ä¶ least squares is another‚Ä¶
1589354394931560448 2022-11-06 20:29:51 +0000 <YiMaTweets> Same here‚Ä¶ as a twitter novice, don‚Äôt know what difference it makes‚Ä¶ can the twitter‚Äôs Chief X Officer care to explain what we get for being ‚Äúchecked‚Äù?
1589343550759915520 2022-11-06 19:46:45 +0000 <YiMaTweets> @no8dee Absolutely, too long to list for a tweet!
1589334192764375042 2022-11-06 19:09:34 +0000 <YiMaTweets> I think 1940s is the true ‚Äúgolden age‚Äù of intelligence: von Neumann‚Äôs game theory, Wiener‚Äôs cybernetics, Shannon‚Äôs information theory, concluded with Turning‚Äôs computing machine and intelligence in 1950‚Ä¶ true wonders of human intelligence! Probably war was the catalyst?
1589303758248607744 2022-11-06 17:08:38 +0000 <YiMaTweets> @Kaszanas For some, it is "easy" to publish...
1589303044961112064 2022-11-06 17:05:48 +0000 <YiMaTweets> Now we know the conference reviewing process is a diffusion process. How should we learn a score-matching function for the inverse denoising process, so that we can converge back to the true value of the papers?
1589137890462031872 2022-11-06 06:09:32 +0000 <YiMaTweets> I think we pretty much have enough evidence that reviews of "top" big conferences are very random. The question is what we are going to do about it.
1589133409854828545 2022-11-06 05:51:44 +0000 <YiMaTweets> These days, what is good about writing machine learning papers is that you only need to convince graduate students; what is bad about writing machine learning papers is that you have to convince graduate students. (no offense to graduate students though).
1589020857422458881 2022-11-05 22:24:29 +0000 <YiMaTweets> To give a talk on Wednesday at Berkeley Eastern Asia Student Organization TOPPA (Japanese for "breakthrough"):  https://t.co/RJny69Uxfx Approaching my record of 40 talks a year set in year 2021...
1589018889715089408 2022-11-05 22:16:40 +0000 <YiMaTweets> To give the Plenary Talk next Friday at the 2nd Michigan State University CMSE Data Science Student Conference (DISC) 2022:  https://t.co/uyYdtFF2eQ
1589018433454485504 2022-11-05 22:14:51 +0000 <YiMaTweets> To give an ESE Department Colloquium at the University of Pennsylvania on Monday:  https://t.co/ekE5ua8SNO
1588616976355962880 2022-11-04 19:39:36 +0000 <YiMaTweets> All settings use the same closed-loop architectures, optimize the same kind of principled objective functions, learn through the same type of feedback control/game  mechanisms. This is arguably the first fully autonomous learning unit.
1588614718427910144 2022-11-04 19:30:38 +0000 <YiMaTweets> As I mentioned before, all future intelligent networks and systems should be closed loops. In fact, all the deep networks used in these closed loops can be replaced by white-box "sparse coding" networks, just like that in our recent NeurIPS paper:  https://t.co/FIaL3ZzZiz
1588613760687968256 2022-11-04 19:26:50 +0000 <YiMaTweets> Our recent coherent series of work on Compressive Closed-loop Transcription under supervised ( https://t.co/F1FO0R85le), incremental ( https://t.co/Ptai0xJzsC), and unsupervised ( https://t.co/FvsLgJNSHt) settings, better performance with smaller networks and less resources.
1588577312320016385 2022-11-04 17:02:00 +0000 <YiMaTweets>  https://t.co/TcSlyhhQRv My paper ‚ÄúOn the Principles of Parsimony and Self-consistency‚Äù was voted the best paper of last month by a Chinese paper reading social website. It was meant to provoke eager young minds‚Ä¶
1588282204777459712 2022-11-03 21:29:21 +0000 <YiMaTweets> Theory and Practice are like a married couple, so do Academia and Industry. Cycles of courting, dating, wedding, birthing, divorcing etc can all find their parodies...
1587712231969501184 2022-11-02 07:44:29 +0000 <YiMaTweets> You go for larger models or bigger datasets only because you are out of good ideas. I always believe that true mechanism for intelligence is the most resource efficient hence democratic: intelligence is not the privilege of the most resourceful. That is not the way of nature...
1587699858692923392 2022-11-02 06:55:19 +0000 <YiMaTweets> The deadline for submitting a paper to the 3rd workshop on Low-Dimensionality and Deep Networks in Abu Dhabi has been extended for another week:  https://t.co/3427OEwaIZ Each accepted paper will receive $1,000 travel grant to attend. A great workshop at at an exotic place!
1587697081635901440 2022-11-02 06:44:16 +0000 <YiMaTweets> Kind of echos the spirit of my first paper about DL back in 2014:  PCANet: A Simple Deep Learning Baseline for Image Classification? (A two-layer PCA networks, purely forward constructed, no BP, and yet SOTA on many  datasets and tasks of the time). Of course rejected many times.
1587694232394833920 2022-11-02 06:32:57 +0000 <YiMaTweets> I have many invited talks lately. Next week two are very special: one by a Michigan State student research conference and one by a Berkeley east Asian student organization. It always a special honor for being recognized by students, who matter the most to impress or to inspire...
1587613381464272896 2022-11-02 01:11:41 +0000 <YiMaTweets> @archieLurker They make most money from ads... the very fact that we use it is helping them make money...
1587567622719098880 2022-11-01 22:09:51 +0000 <YiMaTweets> I do not get it why I need to pay (anything) for sharing information with colleagues... I thought this is not about money...
1587342053683232768 2022-11-01 07:13:31 +0000 <YiMaTweets> Can AI automatically generate annual progress reports for research grants? That would be a life-saver (or at least time-saver) for many faculty.... Wonder whether NSF or DARPA would like to sponsor some program to try to develop such an intelligent reporting system?
1586082324319330304 2022-10-28 19:47:48 +0000 <YiMaTweets> @shai_machnes interesting you believe travel agents and artists have the same job nature...
1586056496206385153 2022-10-28 18:05:10 +0000 <YiMaTweets> To give a distinguished lecture at Hong Kong university Nov. 25:   https://t.co/TAG6W9Srri third time for universities in Hong Kong this year.
1586021646258708480 2022-10-28 15:46:41 +0000 <YiMaTweets> There have been speculations that AI will be replacing artists. Well, current  artificial intelligence (AI) is best interpreted as assisted intelligence (to machine) which is good for assisting intelligence (of human). No worries till machines have fully autonomous intelligence.
1585754914600325121 2022-10-27 22:06:48 +0000 <YiMaTweets> Marked and saved this just in case...
1585742012996718592 2022-10-27 21:15:32 +0000 <YiMaTweets> I would argue both mathematical theory and programming are equally important these days: theory is the brain; programming is the hands. One cannot go far by having only one; and hopeless if having none...
1585724708204339200 2022-10-27 20:06:46 +0000 <YiMaTweets> @milos_ai Given that people are able to discern and judge wisely... instead of being simply influenced by whichever comes first...
1585719692802433024 2022-10-27 19:46:50 +0000 <YiMaTweets> I actually believe regular people are typically much more biased than establishment journalism. Only that they used to have less influence on each other. Now they do through social media like twitter... Quality of a source should be measured by its signal to noise ratio (SNR)...
1585686434148515840 2022-10-27 17:34:41 +0000 <YiMaTweets> Because it is essential? My very first paper was about the  continuous version of essential matrix (I believe most vision people today are still not aware of the differences/caveats). My first 3D Vision book started with this topic too. Well, people will realize, eventually.
1585656436305981440 2022-10-27 15:35:29 +0000 <YiMaTweets> @Singularitarian Thanks. I know this idiom and that is what he wanted to meant. But in this case, with all the extra drama, the stunt could be interpreted as a pun, or something more...
1585496647227740161 2022-10-27 05:00:32 +0000 <YiMaTweets> What does this stunt mean? ‚Äú44 billion down the drain‚Äù?  https://t.co/ftXJzYG8Vg
1585481467945025536 2022-10-27 04:00:13 +0000 <YiMaTweets> Encouraged by the success for classification, we will soon release some work showing the same simple and  constructive networks work strikingly well for generative/autoencoding tasks. Our goal is to make everything white-box: objective, architecture, final representation learned.
1585479716001103872 2022-10-27 03:53:15 +0000 <YiMaTweets> Roughly speaking, one can simply view this as Yann LeCun's Convolution DNN + LISTA. But it is just a little  baffling why this has not been convincingly justified before empirically -- we are by far not the first had tried. Principles work, only need to be implemented correctly.
1585478573627830272 2022-10-27 03:48:43 +0000 <YiMaTweets> This NeurIPS paper is essentially an sanity check: "Revisiting Sparse Convolutional Model for Visual Recognition" ( https://t.co/FIaL3ZhQ4r):  could the simplest sparse coding modules be sufficient to construct deep networks that are just as good for large-scale recognition tasks?
1584426553043128320 2022-10-24 06:08:22 +0000 <YiMaTweets> In math, finding a solution (&amp; verifying it) is often not the most interesting part. Much more interesting is the approach to a solution -- often good insights and new tools are developed that help solving a family of similar problems -- why I do not like black-box approaches...
1584424250852618240 2022-10-24 05:59:13 +0000 <YiMaTweets> Human brain seems not so bad after all, compared to DeepMind's brute-force reinforcement learning:  https://t.co/LDcr4ZoRbP Or maybe the problem is not as hard as previously thought? Regardless, I love the connection of this problem to low-rank tensor factorization, though.
1584033096088723456 2022-10-23 04:04:54 +0000 <YiMaTweets> Cool. Now we need to work seriously with 3D vision...
1583997291307769858 2022-10-23 01:42:38 +0000 <YiMaTweets> Always respect people not only with foresight but also take actions about it... üëç
1583925770170220544 2022-10-22 20:58:26 +0000 <YiMaTweets> There is no conflict if related work is already or going to submit to other international conferences. Besides, top papers have a chance to be invited for a special issue on IEEE transactions on Signal Processing.
1583923174378389504 2022-10-22 20:48:07 +0000 <YiMaTweets> @versatran01 Thanks...
1583922992395874304 2022-10-22 20:47:23 +0000 <YiMaTweets> A reminder: submission to the 3rd Workshop on Seeking Low‚ÄëDimensionality in Deep Neural Networks in Abu Dhabi will be due by the end of the month:  https://t.co/3427OEwaIZ. Accepted papers will be provided a travel fund. Your chance to meet and hear from 20 distinguished speakers.
1583884431403597824 2022-10-22 18:14:10 +0000 <YiMaTweets> Received the first royalty check of my new book‚Ä¶ over 700 copies sold in the first three months‚Ä¶ hope ten times that many of the free copy have been downloaded‚Ä¶ should be useful for everyone who really reads it.
1583878162143842304 2022-10-22 17:49:15 +0000 <YiMaTweets> Often heard people say deep learning has been an equalizer for many fields. I believe a rigorous good theory is the true long-term equalizer‚Ä¶ just because old theories failed to connect to new challenges, it doesn‚Äôt mean we should lose faith in there is a new one.
1583751929548578816 2022-10-22 09:27:39 +0000 <YiMaTweets> Machine learning is the art of reinventing math with diligent trial and error? Only problem is that the same ‚Äútrick‚Äùis often given too many new names‚Ä¶
1583748684210008064 2022-10-22 09:14:45 +0000 <YiMaTweets> Glad to see increasingly solid theoretical connections between sparsity and deep networks‚Ä¶ we will soon release some strong empirical evidence that sparse coding can lead to design of fully interpretable networks.
1583509314777407488 2022-10-21 17:23:35 +0000 <YiMaTweets> I believe, the memory in our brain, for the seemingly complicated and nevertheless predictable external world,  works the same way.
1583508132956762112 2022-10-21 17:18:53 +0000 <YiMaTweets> That is precisely the point of a closed-loop transcription: it is a universal, probably the only,  mechanism for distilling, organizing, and internalizing  essential information from the external (this case phenotypes), to store compactly, to use efficiently, and to pass on.
1583278136220782592 2022-10-21 02:04:58 +0000 <YiMaTweets> We need precise mathematical and computational models for the neural systems.
1583223280651055105 2022-10-20 22:26:59 +0000 <YiMaTweets> @yannx14245717 In a way, yes. The "energy" I am interested, rate reduction, is intrinsic -- the information gain from the representation, a principled measure for the distribution of the desired representation. Of course, one could consider more general energy -- but it needs to be justified.
1583208944444403713 2022-10-20 21:30:01 +0000 <YiMaTweets> Received hard prints of our recent paper from the publisher‚Ä¶ This is one of the few papers I would like to keep a hard copy.  https://t.co/AnT9ccsHLA
1583169069624066048 2022-10-20 18:51:34 +0000 <YiMaTweets> No doubt high-dim statistics and data science are deeply connected -- my new book is about it and many of my own work benefit from the connection. However, I want to point out, their difference is where the main challenges and opportunities lie for the young data science research
1582937046313623558 2022-10-20 03:29:36 +0000 <YiMaTweets> Hence, I always believe directly borrowing methodology from statistical physics to study data science or machine learning is problematic. The true (conceptual and computational) challenges are almost opposite.
1582936399522631681 2022-10-20 03:27:01 +0000 <YiMaTweets> They both use super computing: one is to simulate and draw samples from the distribution; one to fit a  distribution to humongous sets of data samples...
1582936018788810752 2022-10-20 03:25:31 +0000 <YiMaTweets> What are the similarity and difference between statistical physics and data science? They all deal with high-dimensional distributions. But statistical physics often has (form of) the distribution but no samples; data science often has plenty of samples, but not the distribution.
1582666898340134912 2022-10-19 09:36:07 +0000 <YiMaTweets> Two fundamental challenges for learning a distribution in high-dim space: the density and many associated quantities are intractable to evaluate; the density is not even be well-defined if its support is low-dim (zero-measure). The latter one is a blessing, if properly handled...
1582660130197229568 2022-10-19 09:09:14 +0000 <YiMaTweets> @iScienceLuvr Yes, it was formulated more generally, and then denoising with Gaussian was a special case... That was just in statistics. I am pretty sure similar ideas had long been exploited by statistical physicists (in 70's or even earlier)...
1582659498581184513 2022-10-19 09:06:43 +0000 <YiMaTweets> Only young researchers know more about the history of their own fields and those related could they be truly humbled by how difficult, or nearly impossible, it is to come up with something truly original...  Often you may  not even know the right direction or true challenges...
1582655955862372353 2022-10-19 08:52:38 +0000 <YiMaTweets> Maybe because I write textbooks, I have developed a habit of finding history of great ideas: when they first appeared, then evolved, or reinvented in different fields or areas for different reasons. I believe that is the only way one can truly appreciate and respect originality.
1582653659514445824 2022-10-19 08:43:31 +0000 <YiMaTweets> Just learned more history from prof. Eero Simoncelli of NYU: the score-matching type diffusion/denoising methods had long been used for natural image denoising (at least by Eero's group). The method can actually be traced back to Empirical Bayes statistics in 50's (Miyasawa 1961)
1582462646346911749 2022-10-18 20:04:30 +0000 <YiMaTweets> "Great artists steal..."
1582258330738663424 2022-10-18 06:32:37 +0000 <YiMaTweets> @therealBoronik I agree. But I think we are not yet done with learning and organizing the structures/distributions of the raw sensory data, in a fully autonomous and intelligent way. Let alone higher level concepts. Can someone simply tell how the notion of "1" is formed, computationally?
1582222047622492160 2022-10-18 04:08:27 +0000 <YiMaTweets> @cthorrez Yes
1582222012931395585 2022-10-18 04:08:18 +0000 <YiMaTweets> @jagunlabi_m My own investigation into this in the past few years‚Ä¶.
1582221027001573376 2022-10-18 04:04:23 +0000 <YiMaTweets> Many people are talking about artificial general intelligence (AGI), l think l am still working on the special theory of intelligence‚Ä¶
1582215295858376705 2022-10-18 03:41:37 +0000 <YiMaTweets> @lenovo_ai Locally, yes. Per layer, probably true. Concatenated many layers together, globally it can emulate optimization algorithms that learn rich nonlinear structures and complex distributions‚Ä¶
1582213029264912384 2022-10-18 03:32:36 +0000 <YiMaTweets> I never got seriously interested in DNNs till I was forced to teach it at ShanghaiTech in fall 2016. I realized l didn‚Äôt understand it and it seemed nobody else did either. I simply can‚Äôt teach anything unless I understand everything, at least in my own way. Now I think I can.
1581905361916768256 2022-10-17 07:10:03 +0000 <YiMaTweets> Never heard about this trick. I always read from the very beginning and work from there -- I do that even for writing a whole book. I often go back and forth to seek possibly better reorganization... Might give a little try and see what difference it makes...
1581702932604497921 2022-10-16 17:45:40 +0000 <YiMaTweets> @QRJ211 Haha, funny!
1581421452774952960 2022-10-15 23:07:10 +0000 <YiMaTweets> @Sergei_Imaging That is precisely the essence of learning: one simultaneously estimates the underlying structures (submanifolds) of the data distribution and decides how to denoise and deform the distribution wrt such structures. All methods differ slightly on how local structures are measured.
1581365624923516929 2022-10-15 19:25:19 +0000 <YiMaTweets> So far, I haven't seen much that goes beyond the slogan: "We learn to compress, and we compress to learn!" Energy/entropy, coding length, rate distortion, score match, are just different ways to measure the (local) volumes of the data, then you decide to compress or to expand...
1581364016311521280 2022-10-15 19:18:56 +0000 <YiMaTweets> Anyone realizes the (local) expectation of the score match function is exactly the gradient of coding length of the (local) data distribution? That is the same as the gradient of the rate distortion (via lossy coding) -- indicating directions of compressing or expanding the data.
1581362491707183104 2022-10-15 19:12:52 +0000 <YiMaTweets> Hyvarinen's original score matching paper was not meant to do the naive denoising (arguably the simplest case), behind all the diffusion models. The goal was to solve ICA or dictionary learning, which I call structured denoising/diffusion. We'll see what we can do about that soon
1581359724829626369 2022-10-15 19:01:53 +0000 <YiMaTweets> Got many inquiries about our new work. Well, on my personal website, you may find what I have done for that this year: A New Manuscript, A New Presentation, A New Textbook, A New Course, A New Workshop, and A New Tutorial. If you still cannot find a way in, I am out of ideas...
1580802794696503298 2022-10-14 06:08:50 +0000 <YiMaTweets> @lzyang2000 @Caltech Not yet...
1580802722353188864 2022-10-14 06:08:33 +0000 <YiMaTweets> @Gambit3301 @ucl @UniofOxford @Cambridge_Uni Scheduled one at Oxford in July, guess what, the British Government did not issue the visa in time (i applied three months before!) I will give a keynote at DeepLearn in January...
1580643060135837696 2022-10-13 19:34:07 +0000 <YiMaTweets> Actually, we should be careful not to call anything intelligence, let alone general one, till we truly know what we are doing. Otherwise, it would like calling a magician a wizard...
1580615299824394240 2022-10-13 17:43:48 +0000 <YiMaTweets> It seems that this year I might have a chance of breaking  my last year's record of 40 talks... more impressive as many talks this year involves traveling...
1580464644828766208 2022-10-13 07:45:09 +0000 <YiMaTweets>  https://t.co/3V9Du50X4h Around our recent position paper, a talk and roundtable about "the Origin &amp; Nature of Intelligence" with Prof. Doris Tsao of UC Berkeley, Prof. Karl Friston of Univ. College London, Prof. Michael Wooldridge at Univ. of Oxford, Mr. Jeff Hawkins at Numenta.
1579958058985304065 2022-10-11 22:12:10 +0000 <YiMaTweets> Again, we are all working on different pieces of the same big puzzle. While each piece is vital to the completion of the whole puzzle, we should avoid arguing which one is more important or more vital... We should reward who can make more pieces fit together... that is what I do.
1579957036367822848 2022-10-11 22:08:06 +0000 <YiMaTweets> I never claim extant techniques, including our new framework, are for higher-level inferences yet, symbolic,  causal etc. In fact, I always say the current framework is merely at the signals to structures level. Nevertheless, I believe it is compatible to these high-level tasks.
1579744902187200512 2022-10-11 08:05:09 +0000 <YiMaTweets> Just release paper/code of our latest work with Meta AI, "In-Hand Object Rotation via Rapid Motor Adaptation":  https://t.co/48krpWoWFe Amazingly natural gaits of robot fingers can be learned from scratch with only proprioceptive sensing, by closed-loop compression and adaptation.
1579741312181764096 2022-10-11 07:50:53 +0000 <YiMaTweets> @yudapearl @causal_science @Datascience__ @GaryMarcus Also, I do believe systems with self-critique mechanisms such as being able to play game with its model against reality could lead to discovery of relationships beyond statistical correlation (distributions).
1579740722940764160 2022-10-11 07:48:33 +0000 <YiMaTweets> @yudapearl @causal_science @Datascience__ @GaryMarcus Well, the notion of causality has been implicitly modeled and studied in control for dynamical systems for a long time. Maybe not all causal relationships, but many causal relationships are easier to identify with temporal observations.
1579731153661329409 2022-10-11 07:10:31 +0000 <YiMaTweets> Joined twitter about half a year ago, only wanted to promote my new book -- free for download on my homepage. Never imagined hence quite humbled by over 30K followers in such a short time. I guess I will have to find things more valuable to say than what is already in the book.
1579571503934636032 2022-10-10 20:36:08 +0000 <YiMaTweets> @Yubei_Chen PCA is adaptive...
1579381411811069952 2022-10-10 08:00:46 +0000 <YiMaTweets> It is interesting to see so many people/papers are repeating doing exactly the same (incremental) things to diffusion models as they did to GANs...
1579380035697664001 2022-10-10 07:55:18 +0000 <YiMaTweets> @entirelyuseles which one? to me, every one is necessary...
1579379776477093888 2022-10-10 07:54:16 +0000 <YiMaTweets> @rosikand Then the AI program at Stanford is not that different from what is being offered in all other third rate schools worldwide...
1579378644925177856 2022-10-10 07:49:47 +0000 <YiMaTweets> @dmyo_blank They are immersed in theoretical justification but do not care tractable or scalable algorithms, let alone implementing computing tools. Besides, there is unnecessary schism at a time that requires integration with each other for most ML/AI challenges...
1579349970301636609 2022-10-10 05:55:50 +0000 <YiMaTweets> I always believe the few subareas of Electrical Engineering: signal processing, control systems, information theory, optimization already have all the key elements of intelligent systems. But their failure to integrate their tools and make them computable is almost unforgivable.
1579348172350296064 2022-10-10 05:48:41 +0000 <YiMaTweets> @boxquotha start with my homepage... ample resources on compressive sensing and some recent papers on other related topics.
1579346911206985728 2022-10-10 05:43:41 +0000 <YiMaTweets> For all young folks who are interested in intelligence, I highly recommend to study five subjects and understand  how they are related: compressive sensing (sparse coding), information theory, control theory, game theory, and optimization. The rest is mostly realization...
1579338968369598465 2022-10-10 05:12:07 +0000 <YiMaTweets> To some extent, I believe that is exactly what our neural systems strive to do too, implicitly or explicitly: identify predictable statistical or dynamical structures in external observations, then transform and represent them in the most compact (e.g. sparse) forms internally.
1579338245791715328 2022-10-10 05:09:15 +0000 <YiMaTweets> One thing I love about system theory is that it always strives to convert general systems to the most compact and structured forms -- linearized canonical forms. I believe that is a universal and the most powerful way to deal with seeming complex but yet structured world...
1579325122460397568 2022-10-10 04:17:06 +0000 <YiMaTweets> @eigenworld not to trust any content not from trustworthy sources...
1579252260613521409 2022-10-09 23:27:34 +0000 <YiMaTweets> This gives the phrase "Lies, damned lies, and statistics" completely new meaning: texts, images, and videos that  share the same statistics as real ones would be everywhere. One could not trust truthfulness of any such content, unless from certain trust-worthy sources...
1578812004558462976 2022-10-08 18:18:09 +0000 <YiMaTweets> For students, postdocs, and young researchers who like to know fundamental connections between low-dimensional structures and deep networks, try to attend the new workshop SlowDNN, Jan. 3-6, at MBZUAI:  https://t.co/3427OEwaIZ $1,000 travel grant is provided for accepted papers.
1578776746626686976 2022-10-08 15:58:03 +0000 <YiMaTweets> @radenmuaz You should follow principled approaches... instead of follow tricks or folks advocate brute force... if you really need one example, think about how hot AutoML and AutoNAS was once... and where they led us...
1578665470894301184 2022-10-08 08:35:53 +0000 <YiMaTweets> In the practices of modern ML/AI, how many principled ideas were deemed as naive or even bad simply because not implemented properly? Yet, how many mediocre or even bad ideas had convinced the masses, just because enough brute force was put behind? I already knew quite a few...
1578655269185552384 2022-10-08 07:55:20 +0000 <YiMaTweets> @Haoxiang__Wang The CS one this time... In mid December.
1578612077630889985 2022-10-08 05:03:43 +0000 <YiMaTweets> Besides today's lecture, I will also give a few lectures or  colloquiums at UPenn and UIUC later this semester. Hope to reinstall some faith in principles back to the students, and also send a clear message to theory-oriented folks in EE that it is time to integrate their tools.
1578606950530486272 2022-10-08 04:43:20 +0000 <YiMaTweets> Today's lecture was a summation of a tremendous endeavor by my students and colleagues in last 5 years -- a journey to find principled understanding of deep networks. Not only we did that, but also went well beyond -- reward for not following what is popular but what is deeper.
1578605523083018241 2022-10-08 04:37:40 +0000 <YiMaTweets> Gave the UC Davis ECE Distinguished Seminar today and saw many old friends and new students. A day worth marking: first time stated publicly how strikingly well whitebox networks based only on sparse coding and rate reduction work on large real datasets in all settings and tasks.  https://t.co/V2Mu0F1mCT
1578145176035344385 2022-10-06 22:08:25 +0000 <YiMaTweets> Achieving invariance despite nonlinearality, that is a topic Norbert Wiener tried to address in his famed Cybernetics book since 1940's! Now as we know the math works out, let us see invariant sparse coding, compression, and predictive coding in the brain!
1578083346994712577 2022-10-06 18:02:43 +0000 <YiMaTweets> On a more technical level, the "principles" I meant are sparse coding, rate reduction, and closed-loop transcription. They are manifestations of higher level principles of parsimony and self-consistency.
1578082015668146177 2022-10-06 17:57:26 +0000 <YiMaTweets> Can principles lead to purely explainable whitebox deep networks that work the best in all settings: supervised, incremental, unsupervised, classification, generative, and autoencoding, with smaller networks and less data/computation? I will talk about that tomorrow at UC Davis.
1578070797725274112 2022-10-06 17:12:51 +0000 <YiMaTweets> Excited to announce that we will hold the 3rd SlowDNN workshop at MBZUAI in Abu Dhabi first week of 2023:  https://t.co/3427OEwaIZ with a stellar list of 20 confirmed speakers, and good travel support for students with accepted papers. Submission is open till the end of this month
1577849559144747008 2022-10-06 02:33:44 +0000 <YiMaTweets> Great. Finding low-rank decomposition of tensors is hard. This kind of problems are precisely good for machine to search for.
1577406206708023296 2022-10-04 21:12:01 +0000 <YiMaTweets> To give a Distinguished Seminar at UC Davis ECE this Friday. For the first time, I will talk about how well principled purely whitebox deep models work for large-scale image classification and generation tasks, and beyond: Giedt Hall  https://t.co/ZV9pLY6xPP
1577398556515672064 2022-10-04 20:41:37 +0000 <YiMaTweets> Hmmm‚Ä¶
1577392228091117569 2022-10-04 20:16:28 +0000 <YiMaTweets> The name is a little misleading: Learning (identification) and Control (prediction) are in the same closed-loop -- both are equally important and mutually tradeoff and benefit. Not one for the other, nor one is more important than the other. How about just Learning and Control?
1577339081641451521 2022-10-04 16:45:17 +0000 <YiMaTweets> Whoever studying intelligence, the first thing  is to separate the mechanisms that enable intelligence (i.e. ability to acquire knowledge) from the results of intelligence (i.e. knowledge acquired). Machine itself has no intelligence no matter how much knowledge is transferred to
1577336259516563457 2022-10-04 16:34:04 +0000 <YiMaTweets> Very often we are fully convinced that something must be very complicated and mystery is simply because we do not realize there is a very simple and clear explanation behind.
1577334489159651336 2022-10-04 16:27:02 +0000 <YiMaTweets> I believe we are entering the new era of learning with completely whitebox networks and systems made of them -- meaning the mathematical function/operation of each layer and the overall architecture are derivable from optimizing principled objectives hence fully interpretable.
1577197794908991488 2022-10-04 07:23:51 +0000 <YiMaTweets> @mmbronstein @docmilanfar I do not quite know the origin for generalization of the notion of gradient to a field (infinite-dimensional) or a continuous flow... but I believe those are kind of straightforward (conceptually) hence not worth much credit...
1577165376172920834 2022-10-04 05:15:02 +0000 <YiMaTweets> @docmilanfar What is the difference?
1577102155655221249 2022-10-04 01:03:49 +0000 <YiMaTweets> When I teach High-Dim Data Analysis, I always teach  small noise method (aka noisy gradient,  Langevine dynamics, diffusion process), see Lecture 17 of:  https://t.co/iMAz9JOIv4  Students asked when it is useful, I said that is the only optimization method I know works in high-dim
1577099967029596161 2022-10-04 00:55:07 +0000 <YiMaTweets> To my best knowledge, that is due to Augustin Cauchy in 1847, trying to find numerical solutions to equations. It was cited in my high-dimensional data analysis book, Ch. 9. I believe my book did a good job surveying the key methods: gradient descent, Langevine dynamics etc...
1576657543169863680 2022-10-02 19:37:05 +0000 <YiMaTweets> @ron_itelman Yes, all intelligence should follow the sam principles, no matter carbon based or silicon based.
1576570282546200578 2022-10-02 13:50:21 +0000 <YiMaTweets> I pretty much have decided where to spend some time in the next few years‚Ä¶ based upon where I can best amplify the exciting new views that l now hold for the future of intelligence science.
1576565483003731969 2022-10-02 13:31:16 +0000 <YiMaTweets> How many PN-junctions or transistors we need to realize a ReLU operator in a modern neural network, using PyTorch? That factor is at least how much we can gain in efficiency if done at circuit level, of course new circuits with new functions directly for information processing‚Ä¶
1576563326732992512 2022-10-02 13:22:42 +0000 <YiMaTweets> Integrated circuits are integrated systems of transistors, essentially each representing a binary bit. Intelligent systems should be integrated systems of transcriptors, each sorting its inputs to multiple categories.
1576560360731340801 2022-10-02 13:10:55 +0000 <YiMaTweets> I am teaching Berkeley EECS 16b Devices and Systems this semester, which in modern Machine Learning language is ‚Äúeverything you need to know about one layer of a deep network‚Äù. For example, PN-junction, in CS language, is called ReLU, or in signal processing, Soft Thresholding‚Ä¶
1576438031116304384 2022-10-02 05:04:50 +0000 <YiMaTweets> For any iterative optimization algorithm, does anyone expect it to work well with only a couple of iterations? One should expect the same for networks, as they realize optimization‚Ä¶ same reason why it is not so meaningful to consider very deep or very wide networks‚Ä¶
1576436179800846336 2022-10-02 04:57:28 +0000 <YiMaTweets> @DimitrisPapail @SebastienBubeck I meant I understand what DNNs do now, not that conditions under which they all succeed‚Ä¶ it‚Äôs like we know what the purpose of gradient descent algorithms is and how they are built‚Ä¶ but we do not have to know whether it succeeds for every problem it tries to solve‚Ä¶
1576434942979866625 2022-10-02 04:52:33 +0000 <YiMaTweets> @SebastienBubeck I said, to me. Others will catch up later.
1576347898320588800 2022-10-01 23:06:40 +0000 <YiMaTweets> I think l already had perfect new material of real applications for a second version of my new book, if I get to write it in a few years. Nearly ultimate embodiment of the final slogan of the book: ‚Äúwe learn to compress, and we compress to learn!‚Äù
1576345329644949504 2022-10-01 22:56:28 +0000 <YiMaTweets> To me, the only mystery about deep neural networks left is probably why its large-scale practice, both classification and generation, had not been connected to classic sparse signal representation earlier. Principles always work, only need to be implemented correctly.
1576074970664677382 2022-10-01 05:02:09 +0000 <YiMaTweets> After seeing Tesla's robot show today, I wonder how our latest work in collaboration with FAIR and just accepted by CoRL, could work on their hands:  https://t.co/48krpWoWFe (Paper and code are to be released soon).
1576054951599476737 2022-10-01 03:42:36 +0000 <YiMaTweets> Robotics is not easy. But I was hoping for better nevertheless.
1575542439284477953 2022-09-29 17:46:04 +0000 <YiMaTweets> Yup... Obviously, NN are an important component, but not all... We already know there are several other equally, if not more, important mechanisms at play...
1575537515649548288 2022-09-29 17:26:30 +0000 <YiMaTweets> @BlumLenore Regarding "practical" computability, we need to clarify the basic oracles given/allowed for computation. Say access to gradients for optimization (nothing else). Then measuring algorithm complexity based on that.
1575514335983665153 2022-09-29 15:54:23 +0000 <YiMaTweets> @BlumLenore We gave a simple exam in the position paper we wrote... essentially the sensing component needs to have enough room to compare the real from the memorized... low-dim structures make this possible.
1575513927299076097 2022-09-29 15:52:46 +0000 <YiMaTweets> @BlumLenore Even in the case the data structures are linear, the capacity of the sensing/encoding component (e.g. your visual cortex) needs to be at least twice as the intrinsic dimension of the structures (with some additional log factor)... results from compressive sensing of sparse signal
1575197481188478976 2022-09-28 18:55:19 +0000 <YiMaTweets> Surprisingly, not so many machine learning theorists realize that. Computability, the practical one not the theoretical notion of computabilty, is a major factor when it comes to learn higher dimensional structures in data that situate in even much higher dimensions.
1575194252375891968 2022-09-28 18:42:30 +0000 <YiMaTweets> Yann made a key point at Berkeley EECS colloquium yesterday that, learning has to and can only rely on computable entities such as energy, or information quantities (such as volumes, coding lengths/rates, local densities etc. ) Anything on global distribution is prohibitive‚Ä¶
1575191400039460865 2022-09-28 18:31:10 +0000 <YiMaTweets> *The* central mathematical problem for any learning and intelligence is how to learn low-dim submanifolds from samples and transform them to standard forms, using only computable oracles.
1575190410938372096 2022-09-28 18:27:14 +0000 <YiMaTweets> Flying to NYC today, attending a Simons foundation workshop on Mathematical Theory of Deep Learning in the next few days.
1575024075495178240 2022-09-28 07:26:16 +0000 <YiMaTweets> Yes, I did... I remember last time I saw Yann in person was just before the pandemic. We talked about energy minimization and self-supervised learning. What a long way it has come about those subjects since then...
1574593557620326401 2022-09-27 02:55:33 +0000 <YiMaTweets> As we have argued in our recent work, deep neural networks, if designed, organized, and optimized properly, precisely serve as the interfaces between raw sensory data and internal structured representations, which potentially pave the ways for high-level symbolic representations.
1574503872587960320 2022-09-26 20:59:10 +0000 <YiMaTweets> I think everyone is playing with a few pieces of the puzzle: DL gives segments for optimization. But they need clear objectives (info. gain?); they need to be assembled to make (closed-loop) systems; they need other crucial mechanisms for ensuring autonomy and inferring causality
1574237433620680704 2022-09-26 03:20:26 +0000 <YiMaTweets> A few colleagues in China are helping translating my new book to Chinese -- almost done. I volunteered to translate Foreword and Preface... did it last few days and came to realize painfully that my Chinese is getting very rusty... not that I am so smooth with English, either...
1574147535806992384 2022-09-25 21:23:13 +0000 <YiMaTweets> To give a keynote tonight at the Intl. Conf. on Robotics and Computer Vision (ICRVC), China:  https://t.co/ZGsXIxTYJF The number of my talks this year has reached 30, well on its way to break last year's record, 40... Where in the world is that big break I was asking for?
1574131737319178240 2022-09-25 20:20:26 +0000 <YiMaTweets> CongratsÔºÅ
1574100770881871872 2022-09-25 18:17:23 +0000 <YiMaTweets> @WickedViper23 I mean a more dedicated for academic publication. Something that maintain published papers... (not ArXiv as not published, not tweet as too ad hoc, not journals as too rigid, and not conferences as not personal...)
1574098788360212482 2022-09-25 18:09:31 +0000 <YiMaTweets> Do we dare to imagine future academic (open) publication could emulate a site like Tweeter... just publish your papers (through certain review process) and keep tracking how they get read, downloaded, commented, discussed, and cited... You maintain your publications as "tweets".
1574093818122493952 2022-09-25 17:49:46 +0000 <YiMaTweets> I like the feature of Entropy for open access publication: they report and track how many views and downloads. Our paper has been viewed 5500 times downloaded 1400 since published in March.  CTRL: Closed-Loop Transcription  https://t.co/IEiiMV0TV5 #mdpientropy via @Entropy_MDPI
1573824339668963328 2022-09-24 23:58:57 +0000 <YiMaTweets> A central point we are trying to make in our recent position paper about intelligence is precisely to argue what have been missing in the current AI systems and what are necessary for any intelligent agent to be autonomous. Yann's recent position paper shares similar views.
1573816598850203648 2022-09-24 23:28:11 +0000 <YiMaTweets> @rieser_mathsci absolutely...
1573816452255076354 2022-09-24 23:27:36 +0000 <YiMaTweets> Publish or perish, which is true. But publish does not necessarily mean flourish -- especially in an era full of noises. I prefer to publish some to maintain productivity, but spend most of my stamina on projects that I think are important, rather than easy (to publish).
1573813437330640896 2022-09-24 23:15:38 +0000 <YiMaTweets> The so-called top conferences are becoming the slot machines of academia. Please don't make young researchers addictive to them. We all know the process is highly random, but they seem to feel good and even boast the results if they got rewarded. Getting harder to persuade them.
1573599224410710016 2022-09-24 09:04:25 +0000 <YiMaTweets> @ereb0s_labs yes, information can be gained. Two representations for the same low-dimension structures in a high-dimensional space can have different information gains. That is exactly how nature seeks to make the representations more structured...
1573598876245798912 2022-09-24 09:03:02 +0000 <YiMaTweets> @3scorciav @BernardSGhanem @KAUST_News yes, I know him very well. Actually I am on the advisory board that is evaluating KAUST VCC now... Bernard is doing great.
1573416781448097792 2022-09-23 20:59:28 +0000 <YiMaTweets> Oldies but goodies... Although this book was written nearly 20 years ago, it remains ahead of its time even for today, as many fundamental and powerful  constraints in images have not yet been exploited (computationally)...
1573388474048200705 2022-09-23 19:06:59 +0000 <YiMaTweets> One simple law of nature that keeps things simple: simply duplicating what works in the simplest way. That is simply the true genius of nature.
1573387523761512450 2022-09-23 19:03:12 +0000 <YiMaTweets> Maybe, they all share the same (universal) computational mechanisms that distill information from inputs, regardless of the nature of the data? Every cortical column is implementing a compressive closed-loop transcription? The neocortex is an integrated systems of transcriptors!  https://t.co/WYDVqiG1BA
1573381366766764038 2022-09-23 18:38:44 +0000 <YiMaTweets> Before we can explain to a 5-year old, this is my most recent attempt trying to explain (deep) learning and (artificial) intelligence to a broad audience of  neural/cognitive scientists, mathematicians, computer scientists, and electrical engineers. A talk will be released soon.  https://t.co/G8nidv15cv
1573372844079185922 2022-09-23 18:04:52 +0000 <YiMaTweets> If anyone asks the nature of (deep) neural networks, artificial (silicon-based) or natural (carbon-based), my answer is that they are implementing (iterative)  optimization algorithms for compressing the input data or senses, so to gain information.  Nothing else.
1573371173463003136 2022-09-23 17:58:14 +0000 <YiMaTweets> The true nature of all learning (likely intelligence too) is compression. The breadth and depth of the subjects covered in our new high-dim data book seem mind-boggling, but they all summate to the last sentence of the final chapter: "We learn to compress, and compress to learn!"
1573369819797532672 2022-09-23 17:52:51 +0000 <YiMaTweets> "Everything should be made as simple as possible, but not any simpler." -- Einstein
1573055681808285696 2022-09-22 21:04:35 +0000 <YiMaTweets> Congrats!
1572708868492500993 2022-09-21 22:06:28 +0000 <YiMaTweets> @olumidebenjami4 nature did it, once... then it can be repeated...
1572708760459837440 2022-09-21 22:06:02 +0000 <YiMaTweets> @mrbohm_ carbon-based or silicon-based, equipped with similar computing mechanisms that can be self-learning and self-improving till the end of the day...
1572690831349514242 2022-09-21 20:54:48 +0000 <YiMaTweets> @henrycobb It should be able to query the world with predictions based on its current model and learn from how well it does.
1572679682092572672 2022-09-21 20:10:29 +0000 <YiMaTweets> I imagine a new world with truly intelligent systems being one that there will be no digital computers and no need of softwares.
1572679098606157824 2022-09-21 20:08:10 +0000 <YiMaTweets> Prof. Karl Friston made a great point anything intelligent should have the key characteristic of being self-curious. For learning distributions/correlations, I use the term "self-critiquing", and for causality, Judea Pearl calls it "self-interrogating", a key to being autonomous.
1572677862389592064 2022-09-21 20:03:15 +0000 <YiMaTweets> Had a great discussion with Prof. Karl Friston, Jeff Hawkins, Michael Wooldridge and Doris Tsao on the origin and nature of intelligence, following our recent paper on its principles. I do believe computational and scientific studies of intelligence will be able to converge soon.
1572327774748282880 2022-09-20 20:52:08 +0000 <YiMaTweets> @SebastienBubeck I believe there are two separate views: one is to view a network as a function approximator; one is to view a network as an optimizer (that transforms the data distr.). I believe it is the later. Two iterations cannot do much unless the problem is nearly linear.
1572129284885712898 2022-09-20 07:43:24 +0000 <YiMaTweets> May You Rest In Peace!  https://t.co/5bxsonfmZH
1572011575166513152 2022-09-19 23:55:40 +0000 <YiMaTweets> @SebastienBubeck yes, sometimes specific instances could be harder than guidelines for general settings...
1571935838292037632 2022-09-19 18:54:43 +0000 <YiMaTweets> Of course not. But it is undeniably entertaining.
1571935327073501184 2022-09-19 18:52:41 +0000 <YiMaTweets> As we have elaborated well in our new book on High-dimensional Data Analysis -- gradient descent for nonconvex programs could easily become globally optimal when the solutions sought are highly structured (say low-dimensional). Deep networks are precisely seeking such structures.
1571934477374590976 2022-09-19 18:49:19 +0000 <YiMaTweets> @WickedViper23 I have no idea what you are talking about.
1571931671943057408 2022-09-19 18:38:10 +0000 <YiMaTweets> Glad that our recent paper "On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence" made to the cover of the Transactions of Chinese Academy of Engineering.  https://t.co/IPWXff7tHC
1571930131253592064 2022-09-19 18:32:02 +0000 <YiMaTweets> Really feel like that understanding intelligence is like solving a big jigsaw puzzle, as we are all blind men to an elephant. But the good news is, just like solving a jigsaw puzzle, that, the more pieces you have put together, the easier it becomes...
1571925077536706561 2022-09-19 18:11:58 +0000 <YiMaTweets> Science progresses as different areas and perspectives clash, corroborate, and converge. Understanding intelligence requires to "unite and build" ideas from mathematics, computation, and science (neural and cognitive). Glad that our recent paper can help stimulate such a trend.  https://t.co/z4IKwfLY8M
1571366837417553923 2022-09-18 05:13:43 +0000 <YiMaTweets> No doubt that good progress has been made (doesn't matter what to attribute to, data or computing). Nevertheless, something critical is still missing, obviously. So be both optimistic and humble.
1571358009414283268 2022-09-18 04:38:38 +0000 <YiMaTweets> animal intelligence.
1571210651737141248 2022-09-17 18:53:05 +0000 <YiMaTweets> @yudapearl I am a novice in causal inference. Nevertheless, if we consider the *necessary* mechanisms that enable *autonomous* learning of distributions, a proactive mechanism to critique whether the learned model so far is correct is needed -- by playing a game against reality.
1571209315935227907 2022-09-17 18:47:47 +0000 <YiMaTweets> @yudapearl One more thing, my latest conjecture is that such a closed-loop transcription is only a basic unit.  An intelligent system should consist of massive number of such units, each maximizing information gain from its inputs. They are like transistors for integrated circuits.
1571208653004500993 2022-09-17 18:45:09 +0000 <YiMaTweets> @yudapearl The data-fitting optimizers are parts (or segments) of a (closed-loop transcription) system. They together form a learning unit that represents the memory and can continuous to learn and improve. It is possible the self-critiquing mechanism can be used for causal inference...
1571049882571640838 2022-09-17 08:14:15 +0000 <YiMaTweets> If you set your research goal as to publish in the next conference, then it is highly probable you will never amount to anything significant.
1570948040663666688 2022-09-17 01:29:34 +0000 <YiMaTweets> Why this reminds me of storm troopers in star wars?
1570947476588466177 2022-09-17 01:27:19 +0000 <YiMaTweets> @yudapearl @wikibinator @NickRMorgan @tdietterich That is exactly what we intended to do with our recent position paper on intelligence:  https://t.co/gpc4RUqQDW. Starting from necessary assumptions, and deducing *everything* rigorously. Explaining what people have happened done right is merely secondary.
1570900386822909952 2022-09-16 22:20:12 +0000 <YiMaTweets> @yudapearl Most current practices do not as they often use deep networks as a storage (by fitting data). I believe the world model is stored in both the encoding-decoding  networks (as transformations) and the final internal representations (structured and organized).
1570854283217895424 2022-09-16 19:17:00 +0000 <YiMaTweets> Or they got "cherry picked" already? üòâ
1570853041037012994 2022-09-16 19:12:04 +0000 <YiMaTweets> I am glad to see that our recent proposal (with Yann and Doris etc.) about the world model probably unifies and simplifies most of these...
1570846760519741440 2022-09-16 18:47:07 +0000 <YiMaTweets> @yannx14245717 no... I am trying to invent it.
1570846458408206336 2022-09-16 18:45:55 +0000 <YiMaTweets> Majority of our brain is probably learning statistical correlation of the world, hence low-rank part of the  memory; discovery of anything high-level (causal, logical, mathematical, physical laws) are happy sparse accidents, that reflect (aspects of) how the real world works.
1570844718250196994 2022-09-16 18:39:00 +0000 <YiMaTweets> I said this before, there are two types of memory: one is statistical correlations; one is important accidents. We can learn the former from pre-train; we can only learn the later by testing the model against the real-world. I call it the Low-rank plus Sparse memory. pun intended
1570841115175292928 2022-09-16 18:24:41 +0000 <YiMaTweets> @yudapearl I firmly believe that the role of neural networks (artificial or natural) is an interface between the real-world and the internal organized memory. It is NOT a device to store data blindly and then sample from them (though that might already be useful commercially).
1570839634485313537 2022-09-16 18:18:48 +0000 <YiMaTweets> I start to suspect our ability to discover causality might be an accident -- the same self-critiquing mechanism, say via a sequential game, that an autonomous agent employs to interrogate whether it learns the distribution correctly, could leads to discovery beyond correlation.
1570836185353981953 2022-09-16 18:05:05 +0000 <YiMaTweets> I don't believe our  memory is just any way of getting hold of the data distribution: internal representation has to be structured and organized. Not just for easy to use, but also allow agent to manipulate it against the real-world, to discover relationships beyond correlation.
1570834840181940224 2022-09-16 17:59:45 +0000 <YiMaTweets> Why I don't mention causality in our position paper at all: the existing framework for DNNs is ONLY for learning distributions (via stat. correlation) - which is hard enough already. All DNNs I know (including diffusion, transformers) are learning and transforming distributions.
1570833689017450496 2022-09-16 17:55:10 +0000 <YiMaTweets> Absolutely! This is precisely the reason a close-loop is needed -- a necessary mechanism that allows an internally learned model to "self-critique" or to "self-interrogate" itself. This is even needed if an agent wants to learn a distribution right by itself, let alone causality!
1570603126511767552 2022-09-16 02:39:00 +0000 <YiMaTweets> It is always exciting to see new talents grow and mature!
1570575501496037376 2022-09-16 00:49:14 +0000 <YiMaTweets> @SimonParkerson My latest position paper and new book, both are available on my personal website. (and many supporting technical papers and talks, also on my website).
1570540543637323776 2022-09-15 22:30:19 +0000 <YiMaTweets> Important things need to repeat three times: deep neural networks are (nature's) ways to realize optimization algorithms (for compression). The good surviving networks (e.g., CNNs, ResNets or Transformers) are ones that happen to emulate gradient-based algorithms well.
1570539248805376000 2022-09-15 22:25:10 +0000 <YiMaTweets> @ylecun @francoisfleuret That is why we need to study principles, since good and rigorous reasons have been laid in our recent new book on high-dim data analysis -- why/when gradient descent work, and why ResNet is simply one way to realize gradient descent algorithms (for compression).
1570501170166394883 2022-09-15 19:53:52 +0000 <YiMaTweets> Principles might be late, but never absent.
1570499210398826497 2022-09-15 19:46:04 +0000 <YiMaTweets> It is high time the ML and AI community learns to separate fundamental principles and ideas from good practices and engineerings. Early evidence for the former is never going to be as fancy as the later; just like early explanation to success of the later is almost never correct.
1570496404149374977 2022-09-15 19:34:55 +0000 <YiMaTweets> This was an idea many colleagues of mine had or suspected soon after CNNs and ResNet came out. But most colleagues were theoretical-oriented and simply did not engineered things to the point showing convincingly this was the case (on large datasets). Better late than never.
1570495341140779008 2022-09-15 19:30:42 +0000 <YiMaTweets> Our paper "Revisiting sparse convolutional model for visual recognition" accepted to NeurIPS shows a network by unrolling sparse conv. (FISTA, Sparseland like) has the same/better capability than popular CNNs (e.g. ResNet). Principles always work, only need correct engineering...
1570319175222198278 2022-09-15 07:50:41 +0000 <YiMaTweets> What is harder than producing groundbreaking research? Convincing your colleagues about it. What is even harder? Convincing conference reviewers about it.
1570311836310065153 2022-09-15 07:21:31 +0000 <YiMaTweets> @therealBoronik indeed...
1570310194684657664 2022-09-15 07:15:00 +0000 <YiMaTweets> Yes, it is time that we answer those questions clearly. Find the true cause of intelligence and deduce from that. Unification and simplification are not merely for interpretation, but for making true advancement.
1570231485348847616 2022-09-15 02:02:14 +0000 <YiMaTweets> @ManeeshKSingh5 Two (older versions) are, one is on the theory:  https://t.co/AXhqhadVYj one is on incremental learning:  https://t.co/Ptai0xJzsC (results have been significantly improved since). Another one may be released soon.
1570219891512463360 2022-09-15 01:16:10 +0000 <YiMaTweets> Like before, three papers about closed-loop transcriptions have been rejected by NeurIPS, despite compelling results. Well, we will just wait and see when the AI community will realize that is something they cannot avoid doing in every single intelligent system, just like nature.
1570217944092938242 2022-09-15 01:08:25 +0000 <YiMaTweets> @jd92wang @qiqihelloworld Worse had happened to me... multiple times. So do not take these conferences seriously.
1570151855367540736 2022-09-14 20:45:48 +0000 <YiMaTweets> @Gambit3301 quite to the contrary, we predict very well what matters to our survival...
1570141808759545856 2022-09-14 20:05:53 +0000 <YiMaTweets> I believe the world has seen more than enough inductive efforts in the past decade. Deducing a complete computational model of the brain from a handful of computable first principles is an intellectual game I would love to play for the rest of my life. Care to join the quest?
1570139865098096640 2022-09-14 19:58:10 +0000 <YiMaTweets> We may hypothesize that memory of the brain, to a large extent, is a learning machine that maximizes information gain throughout, from low-level continuous sensory inputs, to intermediate structured internal representations, to high-level discrete symbolic and semantic concepts.
1570137575222046721 2022-09-14 19:49:04 +0000 <YiMaTweets> Intelligence is hence a game that any living being constantly plays against nature: to learn maximally what is predictable externally by paying the minimal cost internally.
1570136211083390976 2022-09-14 19:43:39 +0000 <YiMaTweets> Neural networks are nature's way of implementing optimization algorithms. Optimizing what? Nothing but maximizing information gain, in terms of compressing what's similar and contrasting what's different, while minimizing resources, say # neurons (or coding length). Hence a game.
1569036934886096897 2022-09-11 18:55:31 +0000 <YiMaTweets> @sdachen Actually, not true. In almost all my recent papers on learning, I have deliberately used sample sets, pushing populations to the backdrop. All algorithms, even theorems, are stated for sample sets. After all, discrete sample sets are distributions too.
1568896648063102983 2022-09-11 09:38:04 +0000 <YiMaTweets> I tend to believe intelligence is a complete system: it is all about what to do with finite observations, with finite resources, and in finite time. I doubt explaining it even needs to involve infinity or infinitesimal.
1568839649740881921 2022-09-11 05:51:34 +0000 <YiMaTweets> If a matrix is not "general", then it has a "kernel".
1568685387190792192 2022-09-10 19:38:35 +0000 <YiMaTweets> @linty_nlp That is why my recent research on intelligence has focused on deductive methods, deriving everything from first principles (or simply scratch).
1568672742903521280 2022-09-10 18:48:21 +0000 <YiMaTweets> For machine learning: "The reason it worked is due to inductive bias", whereas regarding the inductive bias....
1568670045701799936 2022-09-10 18:37:38 +0000 <YiMaTweets> Indeed, our internal representation of the dependency between motion and (incomplete) visual input must be implicit and non-deterministic.  And just a few occluding contours break that ambiguity...
1568521534184259585 2022-09-10 08:47:30 +0000 <YiMaTweets> I am 50 this year -- known to Chinese as the age of "knowing your destiny." That seems true: just realized  *everything* I have ever studied, learned, and researched on in my life seems to be an integral part of a plausible model for the brains.  That cannot be a coincidence...
1568514038686162945 2022-09-10 08:17:43 +0000 <YiMaTweets> My younger son once asked me why I spent so much time working on writing books and papers. I told him so that he will have good new material to study when he gets to college... instead of learning what I did in college.
1568048461052006402 2022-09-09 01:27:40 +0000 <YiMaTweets> Is intelligence a complete or incomplete system? That is, can intelligence be fully understood by an intelligent system (say our brain) that follows the same set of  principles (or axioms)? Could be there any aspect of it  that the intelligent system can never figure out?
1567980945827045377 2022-09-08 20:59:23 +0000 <YiMaTweets> Year 2022 has been nothing but extraordinary for me: published a humongous book; wrote a significant (to me) paper; and just led and finished a big team proposal. Slept average 4 hours in the past week (explaining why few tweets)... seriously need (and deserve) a big break!
1567053571946090498 2022-09-06 07:34:20 +0000 <YiMaTweets> Artists create, and machines assist.
1565963134342295552 2022-09-03 07:21:20 +0000 <YiMaTweets> Scientists do need to know how to use statistical models and data analysis tools correctly. I have read some scientific papers. To be honest, data analysis conducted in some of the papers scared the day light out of me -- the right jargons, but the wrong math.
1565961579354460160 2022-09-03 07:15:09 +0000 <YiMaTweets> Obviously, we could not publish that paper in any conference of the time (not with that title!), as everyone is yelling on top of their lungs: "bigger and deeper"... It was eventually accepted by IEEE Trans. Image Processing:  https://t.co/3kkeN0EgfX
1565961038943531009 2022-09-03 07:13:00 +0000 <YiMaTweets> In 2014, we used PCA to constructed a two-layer network, purely forward-constructed, no BP, achieved SOTA on all face datasets of the time. The paper is called "PCANet: A Simple Deep Learning Baseline for Image Classification?" It is the true ancestor of the modern ReduNet.
1565846666384576512 2022-09-02 23:38:31 +0000 <YiMaTweets> This shows up everywhere: besides in Kalman filter, also known as Lyapunov map in some context; I found it showed up in using Kruppa's equation for camera calibration, or pose estimation from all types of symmetry, detailed in my Invitation to 3D Vision book.
1565845484379312128 2022-09-02 23:33:50 +0000 <YiMaTweets> @mariotelfig @gabrielpeyre Also Lyapunov map...
1565587271364947968 2022-09-02 06:27:47 +0000 <YiMaTweets> I visited MBZUAI in Abu Dhabi in the summer -- great experience. And we plan to organize a symposium (the third one) on low-dim models and deep networks hosted by MBZUAI in early January.
1565535705001910272 2022-09-02 03:02:52 +0000 <YiMaTweets> We must realize: the type of AI we have been practicing with deep learning in the past ten years is more related to the notion of intelligence that Norbert Wiener meant with Cybernetics at signal/data level, NOT at the (symbolic, logical) level of what McCarthy et. al. meant...
1565057612042842113 2022-08-31 19:23:06 +0000 <YiMaTweets> Looking forward to five talk, keynote, forum, round table, and saloon in September about our recent paper on the principles of  intelligence, all in China. Amongst a big proposal and a big course to teach, this is going to be a crazy September.
1563367700251889665 2022-08-27 03:28:00 +0000 <YiMaTweets> The Toxic Culture of Rejection in Computer Science ‚Äì ACM SIGBED:  https://t.co/Z2nXi10PFB
1563047906772275201 2022-08-26 06:17:15 +0000 <YiMaTweets> Absolutely agree -- never know what that term means, not precisely... Research cannot be done using natural  languages or terms that are not measurable. That is why our predecessors invented mathematics. Don't make ML the art of giving different names to the same vague things...
1562980999918546944 2022-08-26 01:51:23 +0000 <YiMaTweets> I never really delete any of my public comments, except a few later found absolutely incorrect. We should have the freedom of saying something silly once a while... That is what tweets are all about. If want more rigorous or precise, I would just write papers and books...
1562531194436169730 2022-08-24 20:04:01 +0000 <YiMaTweets> Got this as a gift from a student this morning ‚Äî he probably really appreciated my recent position paper that connects intelligence and deep learning back to their cybernetics roots‚Ä¶ it made my day!  https://t.co/4JhD9ksEHp
1562528104945332225 2022-08-24 19:51:45 +0000 <YiMaTweets> This I agree: Language is the result/outcome of intelligence, but not intelligence itself. Intelligence is the mechanism for the formation of individual knowledge and then the formation of language for sharing a portion of it with others -- to improve group intelligence?
1562521201628418050 2022-08-24 19:24:19 +0000 <YiMaTweets> @PMZepto @AiRodeo My recent position paper on intelligence was trying to poke at some of the answers...
1562520792134402050 2022-08-24 19:22:41 +0000 <YiMaTweets> @NewYorkTynes Next year's papers will show how badly past year's papers perform in some situations...
1562343530910978048 2022-08-24 07:38:19 +0000 <YiMaTweets> Black boxes that learn soft statistical correlations are unlikely to give you well organized knowledge hence no precise control. Anyone says we can fix this by adding some new training data is insulting his/her own intelligence.
1562129781570691073 2022-08-23 17:28:57 +0000 <YiMaTweets> @mariotelfig Indeed!
1561977058137219072 2022-08-23 07:22:05 +0000 <YiMaTweets> The new "relational" databases?
1561974368632066048 2022-08-23 07:11:24 +0000 <YiMaTweets> Had a conversation with some students today about what is truly important about research: it is much better to have a wrong solution to a right problem than to have a correct solution to a wrong problem. Problem formulation and justification is far more important than solving it.
1561247845222453248 2022-08-21 07:04:27 +0000 <YiMaTweets> @skalskip92 just try to fix unnecessary schism among fields. why it has anything to do with "power, fame + glory"? never even imagine how my suggestion could remotely be linked to those...
1561244119904636928 2022-08-21 06:49:39 +0000 <YiMaTweets> @therealBoronik read my recent position paper. do not second guess people's motive.
1561234689888661504 2022-08-21 06:12:11 +0000 <YiMaTweets> If folks in control theory, info. theory, game theory, and optimization still don't work together to study intelligence, they will just have to watch ML and AI to reinvent everything they know piece by piece, under different names. All they can do is watching and complaining.
1561073819892076544 2022-08-20 19:32:56 +0000 <YiMaTweets> @tomgoldsteincs I agree. For one, many applications may not need globally optimal solution; Or, some distribution learning problem (with some assistance from human), may not be the worst cases -- gradient can succeed. That happens to the structured non-convex problems we highlighted in the book.
1560751962106126337 2022-08-19 22:13:59 +0000 <YiMaTweets> We should congratulate practitioners for finding working cases. But we should always be cautious when people make claims on generalizability of their methods to a broad class of problems, based on empirical success on a few instances.
1560751088239669248 2022-08-19 22:10:31 +0000 <YiMaTweets> @deliprao gradient descent?
1560750452857139200 2022-08-19 22:07:59 +0000 <YiMaTweets> If you want to bank you future on something principled and predictable, study books like ours; or if you want to bet your future on random luck, keep tuning hyper-parameters and playing datasets...
1560749174710161408 2022-08-19 22:02:55 +0000 <YiMaTweets> These are precisely the most important concepts that John and I have painstakingly trying to convey through the two chapters on optimization in the new book High-dim Data Analysis:  https://t.co/4XEBJXNwqn There is NO free lunch in optimization, but some get lucky once a while.
1560748234108067840 2022-08-19 21:59:10 +0000 <YiMaTweets> Why it works well in many practical cases then? 1. in many of the applications, we do NOT care to learn the globally optimal distribution -- looking good is good enough; or 2. the landscape of the optimization is far from the worst case, no bad locals -- SGD converges globally.
1560746974663438336 2022-08-19 21:54:10 +0000 <YiMaTweets> Like GAN, diffusion process does NOT resolve the intractable nature of learning a high-dim distribution optimally. This class of problems are easily NP-hard. The diffusion method only ensures each incremental step can be efficiently approximated, but no guarantee for global.
1560745762102792192 2022-08-19 21:49:21 +0000 <YiMaTweets> But, do NOT be fooled by its apparent elegance and simplicity. If nonlinear optimization was this simple, we wouldn't have a few hundred years of study.  Devil is elsewhere: its computational complexity is actually exponential for general high-dim nonconvex problems!
1560744922415652864 2022-08-19 21:46:01 +0000 <YiMaTweets> The Diffusion Process that new generative methods (DALLE) rely on is known as Laplace's method (or Langevin dynamics). You can find a systematic intro in Ch. 9 of my new book on High-dim Data Analysis. The proof for its global optimality is only a few lines of college calculus.
1560439721880215552 2022-08-19 01:33:15 +0000 <YiMaTweets> Still trying to get used to papers that claim to be SOTA  methods on generating images and texts and yet not have a single equation in it, except a flow chart. Reading such ML learning papers is itself a challenging NLP and image processing task.  Can AI be of some help here?
1560393025842319360 2022-08-18 22:27:42 +0000 <YiMaTweets> @mariotelfig I have long been criticizing GANs since the formulation is actually not computable to begin with. One of the motivations for diffusion process is precisely the small increments are computable, although still approximately, but much easier to approximate.
1560391825382182912 2022-08-18 22:22:56 +0000 <YiMaTweets> Why transforming distributions to standard structured forms is necessary? Consider learning a linear system. After done with identifying the system with training data, it would still be much easier to design controls once converted to standard (Kalman) forms. Brains do the same.
1560389710337286144 2022-08-18 22:14:32 +0000 <YiMaTweets> I am pretty confident that a closed-loop transcription based architecture, probably in conjunction with an effective distribution learning mechanism  (diffusion+transformation), will ultimately lead to highly controllable and  high-quality generative (and discriminative) models.
1560388293094621184 2022-08-18 22:08:54 +0000 <YiMaTweets> Diffusion models allow one to learn distributions of data and sample from them. But that is not all about learning: the harder part of learning that intelligence needs to solve is how to efficiently compress and transform the distributions into compact and structured forms.
1560386990155018240 2022-08-18 22:03:43 +0000 <YiMaTweets> Absolutely. Similarly, even those tried-and-tested  networks, CNN, ResNet, and Transformer, that survived through hyper-tuning, seem merely  emulating the a couple of lines of math that derive the gradient to change (compress - or expand +) the volume of data/features.
1559784722569388033 2022-08-17 06:10:31 +0000 <YiMaTweets> Just realized a fun fact: it has been 22 years since I got my PhD in 2000. My first textbook took 4 years to write; the second book took 10 years; and the last one took 8 years. They sum up to be precisely 22 years. So on average, I have always been writing my own textbook...
1559782968419241984 2022-08-17 06:03:33 +0000 <YiMaTweets> ‚ÄúMost people overestimate what they can achieve in a year and underestimate what they can achieve in ten years.‚Äù -- Bill Gates. I believe five is more appropriate. So I often set a clear new task every five years or so. So far, it has worked out surprisingly well by the plan.
1559662330643750912 2022-08-16 22:04:11 +0000 <YiMaTweets> The image on my profile is precisely a sum of low-rank matrix and a sparse one, arguably the most fundamentally important statistical and geometric structure -- the model behind Robust Principal Component Analysis:  https://t.co/mRCH4h7TDG
1559661310677098497 2022-08-16 22:00:08 +0000 <YiMaTweets> I have a grossly simplified analogy: What are worth learning about the world is a sum of a low-rank matrix and a sparse matrix. The low-rank part is statistical correlation; the sparse part is something deterministic --  you do not want to learn fire is dangerous statistically...
1559644632174116864 2022-08-16 20:53:51 +0000 <YiMaTweets> @feral_ways That WAS the point! The interpretation of the senses is biased by priors in your brain. You do not need the arrows to have the same effect...
1559644282910257152 2022-08-16 20:52:28 +0000 <YiMaTweets> @oolveea our interpretation of the senses influenced by strong priors...
1559581987744034816 2022-08-16 16:44:56 +0000 <YiMaTweets> @landay mostly remote during the pandemic... hard to say no.
1559383558719807488 2022-08-16 03:36:27 +0000 <YiMaTweets> proof that our brain is Bayesian... and visual, spatial, temporal codings are not fully disentangled. Nevertheless, the learned statistics are good enough (most of the time)...
1559271186332192768 2022-08-15 20:09:55 +0000 <YiMaTweets> Getting approached by many for talks lately. Judging from the rate, it is likely I am going to break my own record of over 40 talks last year. It seems that folks in Asia (China in particular) is paying more attention to our work. Clearly, they like to get any chance to surpass.
1558968242642620417 2022-08-15 00:06:07 +0000 <YiMaTweets> My version: the greatest enemy of new knowledge is not ignorance, it is the complacency of existing knowledge.
1558963557516972032 2022-08-14 23:47:30 +0000 <YiMaTweets> I've a very bad habit in writing proposals: I never tried hard to maximize chance getting funded. I always seek and present new ideas that I would be most excited to work on. Between convincing reviewers and myself, I always choose the later -- writing itself is rewarding enough.
1558959337757757440 2022-08-14 23:30:44 +0000 <YiMaTweets> Following up one Yann's tweet, where is Europe? (although I am very suspicious about the nature of this ranking...)
1558356883529879552 2022-08-13 07:36:48 +0000 <YiMaTweets> @therealBoronik please do better than me...
1558351840768512000 2022-08-13 07:16:46 +0000 <YiMaTweets> Some ask why this journal. Two reasons: it answers a 2-year old invitation from the Chinese NAE; our works along this line have been repeatedly rejected by the "mainstream", including a  paper rejected by an ICML AC despite unanimous accepts by all four reviewers. We need a voice
1558347994419773440 2022-08-13 07:01:29 +0000 <YiMaTweets> Our article "On the principles of Parsimony and Self-consistency for the emergence of intelligence" has been published online in Frontiers of Information Technology &amp; Electronic Engineering. You can view pdf of the paper by using the following link:  https://t.co/t4vLHYMjsV
1558288599916851202 2022-08-13 03:05:28 +0000 <YiMaTweets> @eclecticleaps glad to hear that.
1558175384482394113 2022-08-12 19:35:35 +0000 <YiMaTweets> @penguinvondoom yes, just like "neural network" which once got a bad name for a while (for stupid reasons); "cybernetics" probably should deserve the same...
1558174343099666432 2022-08-12 19:31:27 +0000 <YiMaTweets> I do NOT see any conceptual difference in: encoder/decoder (information theory), sensor/controller (control), analysis/synthesis (signal processing), discriminative/generative (machine learning), wake/sleep (neuroscience)... want to pile up more, and pretend we're progressing?
1558172978348564481 2022-08-12 19:26:02 +0000 <YiMaTweets> Information theory, control theory, game theory, optimization/deep networks, machine learning, and statistical learning, all work on the same set of ideas of  intelligence, just different pieces, and definitely with different terminologies.
1558171504323350528 2022-08-12 19:20:10 +0000 <YiMaTweets> @yannx14245717 how can you be so sure the other big models are optimized properly, or better?
1558152369099001856 2022-08-12 18:04:08 +0000 <YiMaTweets> @subbu10123 start with my latest paper...
1558149594541801472 2022-08-12 17:53:07 +0000 <YiMaTweets> There is this argument that the larger the network model is and the more memorized data/tokens, the better the performance. Well, I want to remind everyone that the largest network is the InterNet. I believe we all experience (or suffer) how intelligent it is, or a lack thereof.
1557991570934075392 2022-08-12 07:25:11 +0000 <YiMaTweets> Personally, I would love to see more papers entitled "XXX alone is not enough," instead of those entitled "XXX is all you need." To intelligence, we are all blind men to an elephant. A more systematic view would make us more humble and the subject of intelligence more intriguing.
1557990604650323968 2022-08-12 07:21:20 +0000 <YiMaTweets> In my recent position paper, to a large extent, we have clarified the role of deep networks/learning. They are the segments (responsible for optimization) in a basic learning unit: a compressive closed-loop transcription. At a larger scope, even transcription alone is not enough!
1557988194049658880 2022-08-12 07:11:46 +0000 <YiMaTweets> I believe Nature is both smart and simple: whatever it passes on to next generations through genes must be very difficult, if not impossible, to learn by individuals in their life-time; whatever it leaves to individuals to learn must permit simple and efficient mechanisms.
1557986815969075200 2022-08-12 07:06:17 +0000 <YiMaTweets> If I were to choose a patron saint for cybernetics out of the history of science, I should have to choose Leibniz. The philosophy of Leibniz centers about two closely related concepts ‚Äì that of a universal symbolism and that of a calculus of reasoning. ‚Äì Wiener, Cybernetics, 1961
1557986496933597184 2022-08-12 07:05:01 +0000 <YiMaTweets> Wiener discussed in his 1948 book the difference and relationship between ontogenetic and phylogenetic learning - birds are mostly the later kind. It remains mystery how some hard-coded intelligence (including genes) has to be learned through selection or can be differentiation.
1557983335460139009 2022-08-12 06:52:27 +0000 <YiMaTweets> Artificial intelligence has mostly been focusing on a technique called deep learning. It might be time to reconsider. | @GaryMarcus in @NoemaMag  https://t.co/4vS6NBQYNc
1557921346549473281 2022-08-12 02:46:08 +0000 <YiMaTweets> Quite honored to be invited to give a keynote tomorrow about my recent work at MILA Professor Chandar's Lab Annual Symposium 2022. @apsarathchandar  It was the intention of my recent papers to have good influence on younger generations of researchers in machine intelligence...
1557292669105164290 2022-08-10 09:08:00 +0000 <YiMaTweets> Really enjoy reading "A Thousand Brains" by Jeff Hawkins. Although not sure if all his theory is correct (leave to neuroscientists), I love the deductive way to approach a seemingly complex puzzle and his faith in something simple and universal... different from the mainstream.
1557216826664239104 2022-08-10 04:06:37 +0000 <YiMaTweets> Watched this clip a few times before. Thinking differently is the only way one even gets a chance to change the world... (for those who do care.)
1557165082970247168 2022-08-10 00:41:01 +0000 <YiMaTweets> Five years from now, all working artificial intelligent systems will be closed-loop, capable of online adaptive learning, just like what in nature.
1557100954733780993 2022-08-09 20:26:11 +0000 <YiMaTweets> @artistexyz I do not think most people, including so-called AI experts, understand correlation and causality. You can have intelligence but you do not have to understand it.
1557096897218678784 2022-08-09 20:10:04 +0000 <YiMaTweets> Learn to decode millions of genes to folding of proteins in 3D is marvelous. But the harder side of the problem is why millions of proteins can be compactly encoded to genes on a 1D DNA. Autonomously learn the closed-loop transcription is true intelligence. Nature did just that.
1557093682221723648 2022-08-09 19:57:17 +0000 <YiMaTweets> @nickinpractice you could start with my recent position paper:  https://t.co/gpc4RUHTFW
1557092046157950976 2022-08-09 19:50:47 +0000 <YiMaTweets> So far, for all practices of machine learning and artificial intelligences in the past decade or so, I have not seen much going beyond computing and utilizing second-order statistics or first-order geometry of data. But the explosion of new names given to them is astonishing...
1557090125451911168 2022-08-09 19:43:09 +0000 <YiMaTweets> "Mathematics is the art of giving the same name to different things." -- Poincare. ML/DL seems to be the art of giving different names to the same thing. The scary part is that many students or experts believe these different newly given names have different mysterious meanings.
1556929200325464064 2022-08-09 09:03:42 +0000 <YiMaTweets> Gave a lecture to Bytedance AI lab summer camp today. Hundreds of people attended. Reminded me of giving a summer course at Bytedance four years ago on similar topics. Only 3 people finished in the end. When something is already popular, you have missed the golden opportunity‚Ä¶
1556858665847508992 2022-08-09 04:23:25 +0000 <YiMaTweets> @QRJ211 Then those are not good recruiters...
1556774070208778243 2022-08-08 22:47:16 +0000 <YiMaTweets> I always tell all my students: do not take outcome of *any* conferences seriously, no matter what others tell you. Focus on doing significant research and writing good  papers. Treat conference submissions as a drill for sharpening your academic skills - that is all what they are
1556744764455735296 2022-08-08 20:50:49 +0000 <YiMaTweets> I agree. And the discussion, if any at all, is pretty non-productive. No intellectual debate, more of a game of words: the reviewer would insist his "apple-to-orange"  is "apple-to-apple", and your "apple-to-apple" is "apple-to-orange." How to argue against "I feel", "I believe"?
1556459827995353088 2022-08-08 01:58:35 +0000 <YiMaTweets> When I wrote the chapter 10 of my Invitation to 3D Vision book  almost 20 years ago, I know sooner or later people will find how important symmetry is for 3D reconstruction. Started to revisit this in the context of learning:  https://t.co/hljqZ0rEWA
1556423806943576069 2022-08-07 23:35:27 +0000 <YiMaTweets> I could never understand why so many (smartest) people could spend their whole life on studying one subarea of knowledge that obviously should be integrated with several others, to become anything truly useful or meaningful at all.
1556392864292081664 2022-08-07 21:32:29 +0000 <YiMaTweets> Just had another "eureka" moment. Had quite a few of those in the past few years when trying to put together the puzzle of intelligence. Hope this one is just as substantial. Will wait and see what happens in a year or so.
1556388723364745216 2022-08-07 21:16:02 +0000 <YiMaTweets> One should never expect to learn anything substantial from blogs! They are at best links to useful resources -- you won't learn much at all by staring at a bunch of web links/titles and guess that their contents might be.
1556387997955678208 2022-08-07 21:13:09 +0000 <YiMaTweets> Many send questions to me by tweets or emails. Forgive me if I do not respond. First, I might be swarmped with things, as always; second, many  questions indicated that you have never read or understood any of my recent book or papers or talks, readily available on my website.
1556006160582422528 2022-08-06 19:55:52 +0000 <YiMaTweets> @jimmy_cyyeh the learning and organizing process is still human assisted and controlled. the algorithms/systems themselves do not have intelligence.
1556005945527844864 2022-08-06 19:55:01 +0000 <YiMaTweets> @yannx14245717 yes, rate reduction is one example.
1555999499146764288 2022-08-06 19:29:24 +0000 <YiMaTweets> When I was getting my Master degree under professor Alan Weinstein at Berkeley, he once told me that the difference between a good mathematician and a great mathematician is that a great mathematician loves to compute directly, instead of searching &amp; building on others' results.
1555996778549325824 2022-08-06 19:18:35 +0000 <YiMaTweets> It is also true by replacing "student" with "researcher".
1555244205877776384 2022-08-04 17:28:08 +0000 <YiMaTweets> Most generative models so far are manipulating the data, which is relatively easy as one only needs to get a rough hold of data distributions (e.g. via GAN or diffusion models). But the real hard part of intelligence is how to transform &amp; organize the distribution to our benefits
1555240614463041537 2022-08-04 17:13:52 +0000 <YiMaTweets> @mmbronstein good ideas... I had been advocating improvements of the conference reviewing systems for vision conferences for years, to no avail though...
1555085222525296641 2022-08-04 06:56:23 +0000 <YiMaTweets> This is the correct order of publishing thoroughly reviewed research findings first and disseminating through conference, as done in mathematics or science... But I doubt it would work effectively with the ML's culture of popularism.
1555078058322735104 2022-08-04 06:27:55 +0000 <YiMaTweets> On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence  https://t.co/TwEtepSyvf Tomorrow August 5, 11am (Hong Kong time), HKUST, Institute of Advanced Study, lecture and round table discussion with Harry Shum and Doris Tsao.
1554972873466384384 2022-08-03 23:29:57 +0000 <YiMaTweets> ‚ÄúInstead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child‚Äôs?‚Äù ‚Äî Alan Turing. That is so true. Once we figure out why and how the most basic intelligence emerged, the rest becomes automatic...
1554632877639274496 2022-08-03 00:58:56 +0000 <YiMaTweets> Good education is the (only) solution to most problems in the world.
1554233549720416256 2022-08-01 22:32:09 +0000 <YiMaTweets> Signal processing, information theory, feedback control, game theory, optimization (now known as deep networks) are all different pieces of any integrated intelligent system. People in all these subareas need rid of all the shallow schism, or obstinate and obsolete.
1554230711397146624 2022-08-01 22:20:52 +0000 <YiMaTweets> @MelMitchell1 there is argument against it:  https://t.co/gpc4RUHTFW
1553995191278661632 2022-08-01 06:45:00 +0000 <YiMaTweets> I learned that from teaching. Students are far more important than NeurIPS reviewers though...
1553970149325037568 2022-08-01 05:05:29 +0000 <YiMaTweets> the same with "deep learning", people confuse an end with a means to an end all the time...
1553917475195981824 2022-08-01 01:36:11 +0000 <YiMaTweets> Found a very interesting pattern in ML conference reviews: for a theoretical paper, the reviewers will ask for experiments instead; for an experimental paper, the reviewers will for sure ask for more experiments, often to compare your apple to other oranges.
1553915112263544833 2022-08-01 01:26:47 +0000 <YiMaTweets> Metaphors are only useful to help people better understand after the truth is found, but they could be harmful, or misleading at least, if used as investigative tools. Regarding intelligence, we are all but  "blind men to an elephant"... even not understand the pieces we touched.
1553913464183091200 2022-08-01 01:20:14 +0000 <YiMaTweets>  https://t.co/vOAkzAaXLF A friend sent me this article. As we argued in our latest position paper: generative and predictive models are two sides of the same circle (loop). Modern machine learning and classical statistical learning can be and should be unified.
1553803405407137793 2022-07-31 18:02:54 +0000 <YiMaTweets> To present a lecture and a round table discussion on intelligence, with Harry Shum and Doris Tsao, at the Institute for Advanced Study of the Hong Kong University of Science and Technology on August 5th:   https://t.co/TwEtepSyvf
1553540203872018433 2022-07-31 00:37:02 +0000 <YiMaTweets> @DrElectronX I believe there is a clear difference between claiming implementation/action of ideas versus claiming credits for the ideas...
1553521233722888192 2022-07-30 23:21:39 +0000 <YiMaTweets> @austin_okray We may continue to use these as excuses. Also, I am talking about journal articles and even books.
1553520800812056576 2022-07-30 23:19:56 +0000 <YiMaTweets> How ignorant and arrogant a person must be to be so  confident that the ideas that just came to him/her with a few weeks or months had never ever been thought and examined by others, arguably more diligent and intelligent people in history or in related fields.
1553517091747090432 2022-07-30 23:05:12 +0000 <YiMaTweets> Increasingly, I found many machine learning papers and work do not know the history of their field nor know results in other related fields -- do not even pay the very basic effort to know and to connect. If you do not respect history, history will not respect you, for sure.
1553093667140710401 2022-07-29 19:02:39 +0000 <YiMaTweets> @fdellaert thanks. told the editor... hope it will be fixed in time.
1553075077305868288 2022-07-29 17:48:47 +0000 <YiMaTweets> @fdellaert It is a coefficient, might be too technical to state in this paper. It‚Äôs in the referenced.
1553074627068383232 2022-07-29 17:47:00 +0000 <YiMaTweets> @DrElectronX It is called game in the paper.
1552919417666686976 2022-07-29 07:30:15 +0000 <YiMaTweets> By suggesting the compressive closed-loop transcription be a universal learning "engine", I already believe a true  intelligent learning system is not supposed to operate at any (optimized) equilibrium. It should always be in the transient: constantly adapting and improving.
1552917187601436672 2022-07-29 07:21:23 +0000 <YiMaTweets> Just updated and posted the final (to be published) version of our recent position paper on intelligence:  https://t.co/gpc4RUHTFW I believe this is probably the best effort yet to fit all the known pieces of the puzzle together. That also leaves the open boundary clear...
1551466718865657859 2022-07-25 07:17:45 +0000 <YiMaTweets> @rezar Learning and understanding things better are the only things that I know that make me feel excited.
1551321269781479424 2022-07-24 21:39:47 +0000 <YiMaTweets> In 2021, I gave over 40 talks about the rate reduction, which was only encoding side of the loop. It seems that I am going to give similar number of talks this year and next about the complete compressive closed-loop transcription.
1551320146295238656 2022-07-24 21:35:19 +0000 <YiMaTweets> This Tuesday (26th) will give a keynote at Hong Kong Tech Forum on Data Science and AI (DSAI):  https://t.co/XCvn4auC4g
1550986382419472384 2022-07-23 23:29:04 +0000 <YiMaTweets> Absolutely -- the temperature is the temper in my brain while doing it. üòâ Only I do it alone, without any backup...
1550894337440894977 2022-07-23 17:23:18 +0000 <YiMaTweets> @GittaKutyniok Congrat, Gitta!
1550651050247344128 2022-07-23 01:16:34 +0000 <YiMaTweets>  https://t.co/TwEtepSyvf Got quite many invitations for talks lately‚Ä¶ to give a distinguished lecture and round table at HKUST Institute for Advanced Study on August 5th.
1550645798307651584 2022-07-23 00:55:42 +0000 <YiMaTweets> @yannx14245717 That is the regularization term added because the size of the sphere/bean you choose to pack the volume. See my 2007 paper (referenced)
1550535456927846404 2022-07-22 17:37:15 +0000 <YiMaTweets> Maryna Viazovska received this year's Fields medal for solving the optimal sphere packing problem for dimension 8 &amp; 24. The Parsimony principle of our recent article suggests that intelligence might just emerge from learning to pack spheres better for data and count the beans...
1550185267138482176 2022-07-21 18:25:43 +0000 <YiMaTweets> @YoujunHu Not if they really understood the prior work. There are plenty new things to discover. That is our faith.
1550184810039033856 2022-07-21 18:23:54 +0000 <YiMaTweets> @mmsay3d Start with the papers I references in my latest article:  https://t.co/gpc4RUHTFW
1550010160369639426 2022-07-21 06:49:54 +0000 <YiMaTweets> @hatatatam Both have been done and released.
1550009799823073280 2022-07-21 06:48:28 +0000 <YiMaTweets> By the end of year, it will be exactly five years since I joined Berkeley. Intellectually it has been the most excruciatingly intense five years of my life. It might be time for me to take a break. I would probably take a leave in Spring and try to sort out what to do next.
1550007536442413056 2022-07-21 06:39:29 +0000 <YiMaTweets> Five years ago, when I joined Berkeley, I planned two five-year tasks: 1. finishing the book on low-dimensional models; 2. understanding what deep networks are all about. Amazingly, the two tasks turned out to be *one*, even leading to a bigger picture that I had not imagined.
1549663564771704832 2022-07-20 07:52:39 +0000 <YiMaTweets> Emmanuel Candes might beg to differ...
1549569863693193216 2022-07-20 01:40:19 +0000 <YiMaTweets> @artistexyz hence the word "model problem."
1549484082664329216 2022-07-19 19:59:27 +0000 <YiMaTweets> Since early 2000's, Rene and I have always believed subspace clustering (also called Generalized PCA by our 2016 book) is at the model problem for all learning. If real-world data are not subspaces, we make them that way via a nonlinear mapping (DNN) -- hence the rate reduction.
1549473478713036800 2022-07-19 19:17:19 +0000 <YiMaTweets> @yannx14245717 did you read my paper?
1549399986466701313 2022-07-19 14:25:17 +0000 <YiMaTweets> @sonkararmish Apparently you did not read my paper.
1549262951684530177 2022-07-19 05:20:46 +0000 <YiMaTweets> If  Evolution of living things is through Natural selection, then the Evolution of networks seems  through Artificial selection. Somewhat remarkably, top survivors, CNNs, ResNets, and Transformers can all be interpreted (in our paper) as emulating gradient of the rate distortion.
1549223353428238336 2022-07-19 02:43:25 +0000 <YiMaTweets> Made small changes to our recent paper based on reviewers' comments, and updated a version on arXiv:  https://t.co/gpc4RUHTFW   Mainly added a few missing references, fixed a few typos, and revised some wording.
1549158872807383041 2022-07-18 22:27:11 +0000 <YiMaTweets> cool
1549118350508208129 2022-07-18 19:46:10 +0000 <YiMaTweets> @BraneRunner never too late...
1549086322287030273 2022-07-18 17:38:54 +0000 <YiMaTweets> @JingzhiWU5 Spend energy to reduce and transform it.
1549084029948833792 2022-07-18 17:29:47 +0000 <YiMaTweets> ‚ÄúTo create order out of chaos based on random drawing of tiles.‚Äù Just heard this comment by Julia Roberts about playing Mahjong. Amazingly insightful‚Ä¶ the same goes for intelligence and life‚Ä¶
1548907329092870144 2022-07-18 05:47:39 +0000 <YiMaTweets> @Dxstiny_R3sist Read my recent paper:  https://t.co/gpc4RUHTFW
1548896077176131585 2022-07-18 05:02:56 +0000 <YiMaTweets> @aniketvartak More leaning on the serious side... actually. That may lead to understanding high-level intelligence, which I do not believe we have any clear answers.
1548891731399020544 2022-07-18 04:45:40 +0000 <YiMaTweets> Four fundamental elements in an autonomous learning machine: 1. compact coding is the objective; 2. deep networks are the optimization scheme; 3. closed-loop is for the error feedback; 4. sequential game is for self-critiquing. What is the fifth element? The reason to learn?
1548883197731037184 2022-07-18 04:11:45 +0000 <YiMaTweets> @Sergei_Imaging Totally agree with sparsity and bayesian part. My recent book and paper are all about those. But I do not know how to compute causality efficiently in high-dimensional space -- conditional independence is already hard enough. Or maybe we do not need solve causality at scale...
1548882766887915520 2022-07-18 04:10:03 +0000 <YiMaTweets> @itsmahdiz It implies that...
1548777231865958401 2022-07-17 21:10:41 +0000 <YiMaTweets> Amazing, while I was typing a tweet... noticed that my followers have reached 20K... Only joined twitter less than four months ago, as my Weibo account (Chinese twitter) gets increasing scrutiny... It is like writing an open personal journal. Hope it is useful to some of you...
1548771059784069120 2022-07-17 20:46:10 +0000 <YiMaTweets> We do not need foundational models for intelligence; we need foundational principles. Machines need intelligence, but humans should work on their wisdom.
1548765189742292993 2022-07-17 20:22:50 +0000 <YiMaTweets> @nataliaduquew @medicostrol @lexfridman Selfishly, here you go:  https://t.co/4XEBJXNwqn
1548764721519546368 2022-07-17 20:20:58 +0000 <YiMaTweets> @drewroz start with my new book, which took me nearly 10 years to read and write. actually any serious book did the job for you on that regard... but still you need to pay your dues...
1548763795731861504 2022-07-17 20:17:18 +0000 <YiMaTweets> My Phd advisor Shankar et. al.  wrote this article about verifiable AI:  https://t.co/Ddu95B1Hv1 But my view on this matter is: the only way we can trust machine intelligence is that machine can convince us that it can learn from its mistakes -- just like how we could trust humans
1548584024053932033 2022-07-17 08:22:57 +0000 <YiMaTweets> Always take my kids to the Presidio torpedo wharf fishing crabs... next time will definitely check out this park!
1548543999807434752 2022-07-17 05:43:54 +0000 <YiMaTweets> I think we now have a pretty solid proposal and direction  for understanding intelligence. But to verify many of the speculations needs time and resource. I may have to try to find both from now on.
1548531497715437569 2022-07-17 04:54:14 +0000 <YiMaTweets> All key ingredients of AI: Computers (Turing), Neural Networks (McCulloch), Feedback Control (Wiener), Game Theory (von Neumann), Information Theory (Shannon) were all introduced in mid 1940's, during or after the WW II. Coincidence or war's catalyst for such innovations?
1548478980323348483 2022-07-17 01:25:32 +0000 <YiMaTweets> Could not join the trip to London with family this week ‚Ä¶ my 11 year-old son remembered l quoted Lord Kelvin in my paper, found his tomb and sent me a picture‚Ä¶ made me feel so much better!  https://t.co/WZvX5bRq3T
1548475735228047362 2022-07-17 01:12:39 +0000 <YiMaTweets> Only you read profusely about a topic (and history of all its related fields), you may realize how many people in the past have been way ahead of you in terms of ideas. I believe Prof. Stanley Osher said once: to be good at optimization, you need first become a historian...
1548474488563441671 2022-07-17 01:07:41 +0000 <YiMaTweets> I always tell my students: if you only read paper published in the past five years, the probability that you will have any ground-breaking idea in your lifetime is nearly zero. The odds is probably less than winning a jackpot on a slot machine in Vegas...
1548411069076422656 2022-07-16 20:55:41 +0000 <YiMaTweets> @henrycobb The current formulation is just about the most basic unit that is capable of learning. For an intelligent system, it could contain many loops. Some could be driven by higher level goals or rewards, as we contemplated for RL/OC. However, we clarify only what we know for now.
1548369933414715397 2022-07-16 18:12:14 +0000 <YiMaTweets> For me, the hardest part of writing our latest position paper:  https://t.co/gpc4RUHTFW was not the humongous effort to unite many pieces together. It was to seek and justify any truly new ideas at all that Wiener had missed in his original Cybernetics. I think we did just that.
1548367073780174848 2022-07-16 18:00:52 +0000 <YiMaTweets> If I were to choose a patron saint for intelligence out of the history of science, I should have to choose Wiener.  For long, Wiener was the only one who realized all key elements in any intelligent system: compact coding, error feedback, game theory, nonlinearity and invariance.
1548218365189140480 2022-07-16 08:09:57 +0000 <YiMaTweets> "If I were to choose a patron saint for cybernetics out of the history of science, I should have to choose Leibniz.  The philosophy of Leibniz centers about two closely related concepts -- that of a universal symbolism and that of a calculus of reasoning." -- Norbert Wiener, 1961
1548128206162538501 2022-07-16 02:11:41 +0000 <YiMaTweets> Some things are just there to stay, such as matter and math. And we are simply passing observers, with our eyes and with our mind, respectively, seeing merely a fraction of them.
1548028710766276609 2022-07-15 19:36:20 +0000 <YiMaTweets> @HouCell Please tell me how to measure OED, exactly and precisely?
1547629517694349315 2022-07-14 17:10:05 +0000 <YiMaTweets> The correct set of principles for intelligence should work equally well on ants, elephants or humans. Similarly they should work equally well for individual innovators, small startups, or Google. Intelligence has been and should always be the most democratic technology in nature.
1547626246221094912 2022-07-14 16:57:05 +0000 <YiMaTweets> Personally, I truly wish our program can convince people from separate fields/communities to work closely together on Intelligence now: information theory, control systems, optimization/deep networks, and game theory, by realizing they are all necessary pieces of a big elephant.
1547624242035863557 2022-07-14 16:49:07 +0000 <YiMaTweets> A universal learning engine is a closed-loop system that ``unite and build'' four necessary elements together: compact coding, feedback control, optimization networks, and self-critiquing game. It is the most basic building block for any autonomous intelligent systems.
1546908925936603136 2022-07-12 17:26:42 +0000 <YiMaTweets> @dt_maher In our paper, we no longer separate artificial intelligence or natural intelligence. If anything/anyone is intelligent, it should follow the same principles and mechanisms.
1546715623165554688 2022-07-12 04:38:35 +0000 <YiMaTweets> @landay yes. also know as Harry.
1546703643692384256 2022-07-12 03:50:59 +0000 <YiMaTweets>  https://t.co/cardTgLqxE My personal view on  Intelligence -- the most difficult jigsaw puzzle I have ever tried to understand my whole life. The paper is not an invitation for debate. Please no rushed comment or argument on this serious topic till you understand everything in it.
1546649647376003072 2022-07-12 00:16:25 +0000 <YiMaTweets> @itsmahdiz most people never
1546621179506540544 2022-07-11 22:23:18 +0000 <YiMaTweets> Everyone, once a while, needs to think about writing a paper with some soul in it, or at least some passion... At least that is what gets me going with my own research.
1546620370622377984 2022-07-11 22:20:05 +0000 <YiMaTweets> Congratulations...
1546601780930236416 2022-07-11 21:06:13 +0000 <YiMaTweets> Finally, finished the two-week summer course on high dimensional data analysis at TBSI:  https://t.co/iMAz9JOIv4 Seven days a week three hours a day. Quite intense. Try not to do it again.  https://t.co/0AaI4qXXa7
1546590350965239808 2022-07-11 20:20:48 +0000 <YiMaTweets> @RuskEating That is why we have to bring the game to an entirely new intellectual level.
1546576200142766080 2022-07-11 19:24:34 +0000 <YiMaTweets> At a university, for research in AI, we cannot compete with big tech companies in scale or in speed,  but we can for sure compete whose idea/approach is more correct.
1546544635832442880 2022-07-11 17:19:09 +0000 <YiMaTweets> @AmnonGeifman Not that easy... you have to change the bibliography input in the main file, be careful with the names of the main tex files, then you realize all figures cannot be in subfolders... etc. All these are NOT explained on arXiv submission site.
1546374455835836417 2022-07-11 06:02:55 +0000 <YiMaTweets> Can't someone write an interface between overleaf and arXiv? One button to submit, instead of spending a lot of time modifying the files to conform?
1546253621905350656 2022-07-10 22:02:46 +0000 <YiMaTweets> @renegadesilicon I was being sarcastic... I did not have a visa (from the British government) to go with the family on vacation... which I applied more than two months ago
1546216932281442305 2022-07-10 19:36:58 +0000 <YiMaTweets> My summer has turned out to be absolutely perfect, for work.
1546216317203517440 2022-07-10 19:34:31 +0000 <YiMaTweets> I have planned a family trip to Britain almost perfectly: finished an intense two-week summer course and a big paper last night. My flight is noon and I'm all prepared to have a nice vacation with family. Only that I do not have a visa! Mom and kids are going, dad keeps working.
1546039949018873856 2022-07-10 07:53:42 +0000 <YiMaTweets> Just finished a paper on a topic that has consumed my past five years and it has taken entire three months to write on a daily basis with two colleagues. I have never spent so much effort and energy on any article in my life. Hope it is worth it. May post it on arXiv tomorrow.
1545943303274708993 2022-07-10 01:29:40 +0000 <YiMaTweets> For any subject, if one does not know its related fields, one cannot predict its future. Intelligence is intimately  related to information, control, optimization, and game.
1545919023476396032 2022-07-09 23:53:11 +0000 <YiMaTweets> For any subject, if one does not know its history, one cannot predict its future. That includes intelligence, artificial or natural.
1545687933457272833 2022-07-09 08:34:55 +0000 <YiMaTweets> It‚Äôs been more than three years that l almost never go to bed before 1am‚Ä¶ a habit since I was finishing up the book. Every night goes to bed feeling like I have truly earned it. Not sure if others ever feel the same‚Ä¶
1545584936471564288 2022-07-09 01:45:39 +0000 <YiMaTweets> The only reason why many things about deep learning seem mysterious is because we are blind men to an elephant -- everyone is holding one piece and thinking that is the whole. Even deep networks are merely one piece in a bigger puzzle of intelligence...
1545584254679015424 2022-07-09 01:42:56 +0000 <YiMaTweets> @DrElectronX We should figure out what makes rat or human intelligent...
1545583414614450176 2022-07-09 01:39:36 +0000 <YiMaTweets> I was intrigued by these phenomena too five years ago, even wrote a couple of papers about it:  https://t.co/7r7gVy3l1G. Now I realize the problem is a little over-blown in a larger picture: Overfitting is never an issue if the sheer purpose of over-parameterization is to compress
1545577738924199937 2022-07-09 01:17:03 +0000 <YiMaTweets> We might have looked at intelligence from the wrong angle. For long, we try to figure out ways to emulate intelligence, artificially. Maybe we should figure out what intelligence is trying to emulate first. Why intelligence and how? Not philosophically, but truly realizable...
1545575602601611266 2022-07-09 01:08:33 +0000 <YiMaTweets> Intelligence is certainly the biggest jigsaw puzzle that I have attempted to put together in my life. It seems that I have been working on pieces of it for my whole life, even before I realized it. For the past two months, I have been trying to figure out what I have known...
1545571731938521088 2022-07-09 00:53:10 +0000 <YiMaTweets> Just realized that summer is already halfway through and yet I have not had any vacation. For the only family  vacation to England next week, I have not received my visa, which I applied two months ago. It makes me feel a little better their prime minister just resigned...
1544342778095681538 2022-07-05 15:29:45 +0000 <YiMaTweets> Intelligence is not the privilege of the most resourceful ‚Äî that has not been the way of nature.
1544342412637507588 2022-07-05 15:28:18 +0000 <YiMaTweets> I always believe if we could discover mathematical and computational principles behind intelligence, nature must follow or emulate too if we have evolved far enough‚Ä¶
1543839889921167360 2022-07-04 06:11:27 +0000 <YiMaTweets> The same goes for designing the Turing test...
1543380608134500352 2022-07-02 23:46:26 +0000 <YiMaTweets> @PeteTanski I believe the purpose is to discover order, if any, and exploit it...
1543380436549718016 2022-07-02 23:45:45 +0000 <YiMaTweets> @paulabartabajo_ Yes, I am writing an article about this topic... following our research with rate reduction...
1543347625302192128 2022-07-02 21:35:22 +0000 <YiMaTweets> Life and intelligence are all about discovering and  reproducing order, nothing else.
1543321155242643457 2022-07-02 19:50:11 +0000 <YiMaTweets> Increasingly strongly realize that Intelligence is all about discovering order in a seemingly disorderly world. As entropy of every part of the world is always increasing, the occurrence of natural Intelligence must be an incredible fluke...
1541773239776555009 2022-06-28 13:19:19 +0000 <YiMaTweets> I completely agree that much of learning (especially  perception) is not directly driven by any specific external reward/task. Instead, it is something intrinsic (self-consisten) and serves more general purposes. Also, it is time to acknowledge model-based optimal control more.
1541308110068801537 2022-06-27 06:31:04 +0000 <YiMaTweets> I do not think we will understand Intelligence at higher symbolic (language) level until we can understand its  purpose, say  maximizing any measurable gain in group ability or reward by sharing knowledge through highly band-limited communication channels between individuals.
1541306003714560001 2022-06-27 06:22:42 +0000 <YiMaTweets> What about simply "Intelligence"? Something that both nature and machine have to follow if they get smart enough? I believe the practices of AI so far has not gone much beyond Cybernetics, laid out by Nobert Wiener, trying to get any structures at all out of raw data first...
1539478950740733952 2022-06-22 05:22:38 +0000 <YiMaTweets> Next Monday will start teaching a summer course on ‚ÄúHigh Dimensional Date Analysis‚Äù for Tsinghua-Berkeley Shenzhen institute, three hours a day everyday for two weeks. Three years in a row remote teaching‚Ä¶ really miss the fresh lychee in Shenzhen.
1539474990675918848 2022-06-22 05:06:54 +0000 <YiMaTweets> Abu Dhabi is amazing‚Ä¶  https://t.co/B6ZMldb034
1538277201929306112 2022-06-18 21:47:19 +0000 <YiMaTweets> Flying to Abu Dhabi today and visit MBZUAI next week. Many first times this time: flying Emirates, visiting Dubai ‚Ä¶ family is tagging along too. Would be fun.
1537921483787292676 2022-06-17 22:13:49 +0000 <YiMaTweets> I normally avoid philosophical debates -- I would prefer argue things with proofs or evidences. I only trust arguments that are provable or measurable. All those philosophical debates since 50's, 60' did not get us far. In fact, I believe we are more close to Wiener in the 40's.
1537698128840732672 2022-06-17 07:26:17 +0000 <YiMaTweets> I submitted my first paper to CoRL this year. I am going to encourage my students to submit to smaller, more focused conferences from now on... Conferences with too many submissions and two diverse topics are doomed to be mediocre...
1536333381738319874 2022-06-13 13:03:16 +0000 <YiMaTweets> In NYC, attending the Flatiron institute workshop on:  Challenges and Prospects of ML for the Physical Sciences  https://t.co/my07TIaSts
1535880190663876608 2022-06-12 07:02:27 +0000 <YiMaTweets> I am actually visiting MBZUAI in about a week... will find out how my old friend Eric is doing... Never been to that part of the world. It would be fun (and hot)...
1535360490666283008 2022-06-10 20:37:21 +0000 <YiMaTweets> Here is the website (with slides) for ICASSP22 short course on Low-dim Models for High-dim Data:  https://t.co/RCdf0bgYLM  You may view this as a ten-hour overview of the one-semester course EECS 208:  https://t.co/iMAz9JOIv4 We will have videos soon once done recording...
1535346820368375808 2022-06-10 19:43:01 +0000 <YiMaTweets> @allonsygamma Well, it is the country's image. Based on the visa application alone, the image is already worse than many third world countries that I have been to.
1535332111988555776 2022-06-10 18:44:35 +0000 <YiMaTweets> Will be in NYC next Monday and Tuesday attending a Flatiron institute workshop on Challenges and Prospects of ML for the Physical Sciences:  https://t.co/my07THTh4S
1535329814134943745 2022-06-10 18:35:27 +0000 <YiMaTweets> What happened to visa application to the UK? Consulates do not do in person. All inquiry phone numbers (SF and NYC) get routed to the same number in UK which charges money to talk to a person, who knows nothing. Then find out even email inquiry charges money too! A strange county
1535164896354000896 2022-06-10 07:40:07 +0000 <YiMaTweets> @therealBoronik The hard part is how to put all in a unified, efficiently computable, framework. Not hidden with concepts or quantities that are intractable.
1535164391712124928 2022-06-10 07:38:07 +0000 <YiMaTweets> @liuyao12 Yes, but they couldn't.... in fact, to my understanding now, deep learning has more to do with Wiener's program than the latter "AI"...
1535164044465668096 2022-06-10 07:36:44 +0000 <YiMaTweets> @joaoli13 @OpenAI I do not believe a word of this excuse... so did thermodynamics and quantum dynamics to people.. simple laws or principles are usually hiding behind complex phenomena...
1535163471351799809 2022-06-10 07:34:28 +0000 <YiMaTweets> @ndronen lack of a precise computable mind...
1535156043793068033 2022-06-10 07:04:57 +0000 <YiMaTweets> @IamHuijben See my previous tweet?
1535118287654166528 2022-06-10 04:34:55 +0000 <YiMaTweets> I will probably record the talk myself when I have time, if the organizers do not release the recordings. Including the lectures for our ICASSP short courses on low-dim models and deep networks -- my colleagues and I will  make a website for it soon.
1535117526421544960 2022-06-10 04:31:54 +0000 <YiMaTweets> After the plenary at ICASSP'22 and keynote at CITW last week, I got many requests for the videos and slides. As for the video, that is up to the conference organizers and I will post once it is available. Here is the deck of slides of my talk:  https://t.co/ahwh9FqNvC
1533505486846275584 2022-06-05 17:46:13 +0000 <YiMaTweets> The best way to think about gradient descent is to think about a landscape shaped like a tree: thinking about water from rain always reaches the ground‚Ä¶ but if you‚Äôre worst-case person, think turning the tree upside down. Then you start to lose all faith in gradient descent‚Ä¶
1533501002304802816 2022-06-05 17:28:24 +0000 <YiMaTweets> One last thing, another class of far more efficient iterative algorithms that are often mistaken as gradient descent is *power iteration* from contracting mappings! I believe they‚Äôre the most powerful for high-dim data: can be dimension-insensitive for pursuing low-dim structures
1533499126611968000 2022-06-05 17:20:57 +0000 <YiMaTweets> @technotweet Effective yes, but not necessary the most efficient‚Ä¶ nature can afford the time.
1533498602680483841 2022-06-05 17:18:52 +0000 <YiMaTweets> There‚Äôs huge difference between gradient descent algorithms and gradient descent algorithms‚Ä¶ some can be magnitudes faster than others, even to the same class of problems.
1533494119829426177 2022-06-05 17:01:03 +0000 <YiMaTweets> One more thing, as l learned from John Wright, it‚Äôs not gradient descent itself is all powerful, it is the classes of problems we apply them have structures and are amenable to be solved by gradient descent‚Ä¶ he convinced me to add chapter 7 to our book.
1533493111950692352 2022-06-05 16:57:03 +0000 <YiMaTweets> According to my understanding, feed-forward deep networks are emulating gradient descent themselves, although their fine-tuning is just another level of gradient descent‚Ä¶ we made that crystal clear in my new book‚Ä¶ or the ReduNet paper.
1533319945647296513 2022-06-05 05:28:57 +0000 <YiMaTweets> Just because you have implemented some concept or idea in a computer, it does not mean it is computable in the more general sense. It could be intractable for high-dim and large-scale problems, and you only show an instance with brute force or some lucky heuristics...
1533313367078686721 2022-06-05 05:02:48 +0000 <YiMaTweets> I started to realize all work I has done in recent years is to put together the same pieces that Wiener had  studied for the puzzle of intelligence, only made them all measurable and  computable together... This might have not been an accident (as master Oogway always says...)
1533312210943979520 2022-06-05 04:58:13 +0000 <YiMaTweets> Rereading Norbert Wiener's famed book Cybernetics, published in the 1940's... Man, this guy has got *all* the right pieces of the puzzle for an autonomous intelligent systems: nonlinearity, feedback, information, game theory for learning, and brain waves &amp; spikes... Unbelievable!
1533311118638129152 2022-06-05 04:53:52 +0000 <YiMaTweets> While writing a survey paper on my recent work, start to read some of the classics on artificial intelligence... say book "the Sciences of the Artificial" by Hebert Simon. Start to realize the reason why many AI papers these days rarely have math formulae...
1533200427390337024 2022-06-04 21:34:02 +0000 <YiMaTweets> Tomorrow going to Ottawa to give a Keynote at the biennial Canadian Workshop on Information Theory (CWIT)  https://t.co/vbZ4eK6kJh I always believe information theory should have much more influence on modern data science and machine learning, beyond classic communications...
1532623255558713344 2022-06-03 07:20:33 +0000 <YiMaTweets> This sounds like developed countries ask developing countries to cut carbon footprint... Regardless, it is high time to change the brute-force try-and-error approaches to AI. It starts to look less and less intelligent...  Again, resources go to the denominator of IQ.
1530833339778641920 2022-05-29 08:48:04 +0000 <YiMaTweets> Often asked by students why l write books. Well, l don‚Äôt know about other people, but if l do not go through everything myself and get all details organized the way l think making sense, l am never confident in teaching anything‚Ä¶
1530823732700164097 2022-05-29 08:09:53 +0000 <YiMaTweets> Given all the exciting new developments around learning low-dim models, finally decided to write something this summer: something high-level that clarifies basic concepts and ideas before they get misunderstood or carried away, and something technical that unifies many practices.
1530382911404244993 2022-05-28 02:58:13 +0000 <YiMaTweets> @James79093759 which problem?
1530382754788954112 2022-05-28 02:57:36 +0000 <YiMaTweets> @chmod_yes PCA, GPCA, Noninear PCA.... Linear Systems... Linear Transforms and Dictionaries... Info/coding theory for Gaussians... how many more do you need?
1530382189396865026 2022-05-28 02:55:21 +0000 <YiMaTweets> @sethkarten which problem is not? do you know the origin of RL?
1530381990746148864 2022-05-28 02:54:34 +0000 <YiMaTweets> @simonkmtse yes, I found Uber does not work there... but it is too late to set up their version.
1530229699753828352 2022-05-27 16:49:25 +0000 <YiMaTweets> Left Singapore and landed back in San Francisco‚Ä¶ realized need antigen test just the night before an early flight. Turned into a little adventure: took 40 minutes to find a cab from the clinic late at night ‚Ä¶ Singapore taxi system not so friendly to visitors.
1529985583585669120 2022-05-27 00:39:23 +0000 <YiMaTweets> If these classic fields do not extend and engage, ML/DL/RL will continue to reinvent many wheels from these areas and name them differently‚Ä¶ doesn‚Äôt matter you like it or not, till someone write an book to straighten all out, many years from now.
1529983754319679505 2022-05-27 00:32:07 +0000 <YiMaTweets> Classic fields like signal processing, information theory, and control systems should realize by now all their powerful principles and classical models and methods are just one nonlinear transform (say a deep network) away from applicable to ALL modern problems that ML/RL solve.
1529981295148793856 2022-05-27 00:22:21 +0000 <YiMaTweets> Finished ICASSP22 plenary and short course on low dimensional models and deep networks, now back to SFO. Our new messages seem to resonate well with the signal processing community‚Ä¶ Next mission: a keynote at Canada information theory society biannual conference, June 5 to 8.
1528576120668717056 2022-05-23 03:18:41 +0000 <YiMaTweets> @mattdsegal Induction and deduction always work together in science, mathematics, and even medicine for thousands of years... so should be for intelligence...
1528574610337579008 2022-05-23 03:12:41 +0000 <YiMaTweets> "What I cannot create, I do not understand." ‚Äì Richard Feynman. Are we at a transition point that the studies of artificial intelligence will be deductive rather than inductive? What are the necessary and sufficient set of assumptions and principles? Identify them one by one...
1528573097896714240 2022-05-23 03:06:40 +0000 <YiMaTweets> Here is the information about the ICASSP'2022 short course on "Low-Dimensional Models for High-Dimensional Data: From Linear to Nonlinear, Convex to Nonconvex, and Shallow to Deep":  https://t.co/grBkeBzGQJ
1528572310965600256 2022-05-23 03:03:33 +0000 <YiMaTweets> This week at ICASSP'22 in Singapore: besides an opening plenary talk on "Closed-Loop Data Transcription via Rate Reduction", I will also give a short course lecture on "Design Deep Networks for Pursuing Low-Dim Structures", yes, all whitebox network designs from basic principles.
1528162414851928064 2022-05-21 23:54:46 +0000 <YiMaTweets> Congratulations, Dawn! Now by my counts, there are currently 7 or 8 academic brothers and sisters currently being department chairs or deans‚Ä¶  https://t.co/k9kM45inCa
1528160671875366912 2022-05-21 23:47:50 +0000 <YiMaTweets> View from my window in Singapore, breathtakingly beautiful. Here l am ICASSP22!  https://t.co/LuyVPUPQVm
1527746053847470080 2022-05-20 20:20:18 +0000 <YiMaTweets> @seth_stafford @Dr_MehmetSimsek Well, figure out the reason what purposes these branching conditions serve... everything exists (after selection) for a good reason...
1527745643942293504 2022-05-20 20:18:40 +0000 <YiMaTweets> @Dr_MehmetSimsek Or formal assumptions and deductions. Hence "from first principles"...
1527719874616627200 2022-05-20 18:36:16 +0000 <YiMaTweets> What is truly "interpretable AI"? My understanding is: you only have to reveal the fundamental principles, instead of open sourcing your codes. Anyone who understands these principles would be able to deduce, reproduce or produce the best possible results...
1527377142891196439 2022-05-19 19:54:22 +0000 <YiMaTweets> Will head to Singapore tomorrow for ICASSP'22 to give an opening Plenary talk. My talk will be based on two fundamental principles for natural or artificial learning, from two recent  papers: 1. Compression:  https://t.co/CN7eurqo2w ; 2. Self-consistency:  https://t.co/a185jA2OxG
1527375753196646402 2022-05-19 19:48:51 +0000 <YiMaTweets> Ok, seven papers submitted to NeurIPS. All done in time. Will share some of the interesting findings in days to come.
1526623493369913344 2022-05-17 17:59:38 +0000 <YiMaTweets> What else do you expect?
1526444901759598592 2022-05-17 06:09:59 +0000 <YiMaTweets> Trying to survive these two weeks: finishing up two courses, a MURI whitepaper, quite a few NeurIPS papers, as well as a Plenary talk, one 5-lecture Short Course, and one Panel, all at ICASSP'22 next week in Singapore...  seems like a good start for a productive summer...
1526250173889884160 2022-05-16 17:16:12 +0000 <YiMaTweets> @JunchengLi We are talking about academia, not industry... well, precisely this kind of attitude let things slide and eventually unrecoverable.
1526046766038036481 2022-05-16 03:47:56 +0000 <YiMaTweets> My problem is that a much better prepared paper with very positive reviews got rejected and a paper with dubious reviews accepted. That is worse in a way: the decision seems entirely in the hands of ACs...  and my students are loosing faith in me :-)
1526015974838718465 2022-05-16 01:45:35 +0000 <YiMaTweets> @jasondeanlee Honest, I won't mind my papers rejected for constructive comments or decisions with some consistency with the reviews. The meta comments can hardly count as "excuses", for papers that go very good reviews BTW.
1525948840158801920 2022-05-15 21:18:48 +0000 <YiMaTweets> @giffmana @LChoshen All reviewers are actually very positive and favorable about the novelty. Since it is new, some reviewers wanted to see how it works in some settings, we did what was asked. The AC used that as an excuse to reject...
1525897162470477824 2022-05-15 17:53:27 +0000 <YiMaTweets> @RobertoParPal I am a club of my own...
1525893915810770944 2022-05-15 17:40:33 +0000 <YiMaTweets> Why so many papers produced and submitted to "top" conferences: all people see quality of accepted papers, then feel they got chances with theirs, then a loop of positive feedback goes unstable. I was afraid to submit any papers till year 3 of my Phd, knowing what is publishable.
1525767401719095296 2022-05-15 09:17:50 +0000 <YiMaTweets> @roydanroy Yes, we did and will do too this time. Last year ICML, a four-accepts paper but rejected by AC is this JMLR paper:  https://t.co/CN7eurHZr6
1525760495856193542 2022-05-15 08:50:24 +0000 <YiMaTweets> @ShaowInSakura Some of my colleagues are doing that... started some smaller more focused conferences, such as MSML. We could certainly support those from now -- can't be worse...
1525758683434536960 2022-05-15 08:43:12 +0000 <YiMaTweets> @falsalem76 yes. that is why I told my students conferences are just for practicing academic skills. even so, ICML seems clearly worse than others...
1525756606046035974 2022-05-15 08:34:56 +0000 <YiMaTweets> @tfburns Face it, for nearly 10,000 submissions, there is NO quality control. I told my students to submit papers to conferences simply as practicing academic skills.
1525755377085583361 2022-05-15 08:30:03 +0000 <YiMaTweets> My biggest fear with sending papers to venues with  arbitrary reviews and decisions: after a few encounters, my students will complete distrust my research taste and standards, and start to accommodate those of random reviewers and ACs...
1525754142379024386 2022-05-15 08:25:09 +0000 <YiMaTweets> @mmbronstein Me TOO! it is just detrimental to the students! I am afraid, after a few rounds like this, they will stop to trust our research taste and standards now!
1525747925501956097 2022-05-15 08:00:27 +0000 <YiMaTweets> @GilSaqlain yes, we have offered to submit the code or release it public.
1525747670815412224 2022-05-15 07:59:26 +0000 <YiMaTweets> Without reviewers' scores, the ICML reviewing process simply becomes a blackbox for the ACs to play. At least deep networks are black boxes with quantifiable outputs. ICML review process has become a blackbox without. I would not condone such a process.
1525745172310462464 2022-05-15 07:49:30 +0000 <YiMaTweets> Given my last year experience with ICML -- a paper that was accepted unanimously by four reviewers but eventually rejected by the AC without any concrete evidence, plus this year's, I think I will no longer submit any papers to ICML in the future, not with my name on it.
1525744513590841344 2022-05-15 07:46:53 +0000 <YiMaTweets> ICML meta reviews are just arbitrary! In rebuttal we did exactly what reviewers asked, but AC rejected saying: "it is not certain whether this new experiment performed in a short period of time was done accurately."  If can reject by simply not trusting rebuttal, why having it?
1525687790062645248 2022-05-15 04:01:29 +0000 <YiMaTweets> shallow is for mostly linear structures; deep is for nonlinear transformation; trying to model nonlinearity  via a "shallow" model: try to guess a kernel.
1525630308808159232 2022-05-15 00:13:05 +0000 <YiMaTweets> @Ludiusvox Statistically maybe. But I am a Chinese...
1525555886516604928 2022-05-14 19:17:21 +0000 <YiMaTweets> Between inductive and deductive paths, which one is harder? Every inductive step, however small, can be sold as SOTA, whereas every attempt to a deductive framework before success is a failure. One is easily falsifiable, one is hardly ever... The right path is the difficult one.
1525550958863716352 2022-05-14 18:57:46 +0000 <YiMaTweets> @Dr_MehmetSimsek There is value in induction of course, if the purpose is to lead to general principles, but just not induct for the sake of induct... We have recently tried to avert that... it would be an uphill fight, but worth it.
1525547548592680961 2022-05-14 18:44:13 +0000 <YiMaTweets> @ArmesKharesh you can already download a pdf from my personal website... or you may want to buy a hardcopy.
1525546646750105601 2022-05-14 18:40:38 +0000 <YiMaTweets> In the last chapter of the book with John Wright, which connects deep networks to low-dimensional models, we have attempted to bring back some of the deductive thinking, started with a quote: "What I cannot create, I do not understand." -- Richard Feynman (his last words)
1525543990925201408 2022-05-14 18:30:05 +0000 <YiMaTweets> Don't we realize that research in AI has become highly inductive (why so many papers/instances)... the same methodology as Chinese medicine for thousands of years (why so many recipes). What happens to the glorious deductive thinking? Pillar to Science/Math since Socrates...
1525355933945106433 2022-05-14 06:02:49 +0000 <YiMaTweets> Very often we know and say the right thing, but do the opposite... especially as a group.
1525210358582943745 2022-05-13 20:24:21 +0000 <YiMaTweets> Congratulations! Alumnus of the year, UC Berkeley.  https://t.co/dvXUnOiZpF
1525209426570792960 2022-05-13 20:20:38 +0000 <YiMaTweets> @CherWangHTCcom3 Congratulations, Cher Wang won the Alumnus of the Year from UC Berkeley! Thanks for your support of all these years, since we worked together to build ShanghaiTech.
1524903495358222336 2022-05-13 00:04:59 +0000 <YiMaTweets> Interesting story about Harm Derksen: at UIUC, I was interested in computing coefficients of Hilbert functions for subspace arrangements, using the all-powerful supercomputer at NCSA -- quite proud of that. Till one day, Harm humbly told me: he found a closed-form formula...
1524826628458565632 2022-05-12 18:59:32 +0000 <YiMaTweets> Just had a two-hour discussion with prof. Harm Derksen, my original partner in crime on the lossy coding work (while he was at UMich Math):  https://t.co/hmAwUm9m8Z an expert on algebraic geometry of subspace arrangements. Hope we can join force again to generalize to submanifolds
1524630004054646786 2022-05-12 05:58:13 +0000 <YiMaTweets> Cause theory papers are like cooking recipes; whereas empirical papers are like cooked dishes. One is for repeatability; the other is for one-time consumption. Kind of like journal papers and conference papers: one is called "archival", the other is called "one-time" publication.
1524595051346628608 2022-05-12 03:39:20 +0000 <YiMaTweets> I heard a dear friend of mine is going to lead Upenn's new Data Science institute?
1524510776219738112 2022-05-11 22:04:27 +0000 <YiMaTweets> @MingsChirps @Mark__Gilroy Read my old tweets. So far, we believe parsimony (compression) and self-consistency (closed-loop) are two... not sure if there is a third or more.
1524460827146813441 2022-05-11 18:45:58 +0000 <YiMaTweets> @Mark__Gilroy "God tirelessly plays dice under laws which he has himself prescribed." -- Albert Einstein. The transition between randomness and determinism is just a phase transition away, via measure concentration...
1524446343162261504 2022-05-11 17:48:25 +0000 <YiMaTweets> Could not agree with this more! My new book first chap starts with the quote: "Entities should not be multiplied without necessity" by W. Ockham; and the last chap ends with a slogan: "We learn to compress, and compress to learn!" If not all, compression is most of intelligence.
1524443743356555264 2022-05-11 17:38:05 +0000 <YiMaTweets> Congrats, Raluca! Still remember a dinner conversation with her and David Patterson how truly secure and distributed technology can be used for offering future reliable and accessible voting methods -- for places where democracy is denied for lack of such...
1524123970072563712 2022-05-10 20:27:25 +0000 <YiMaTweets> Good to see more learning systems are incorporating multi-view geometry for 3D...
1524122902697676800 2022-05-10 20:23:11 +0000 <YiMaTweets> To repair the increasingly fragmented world of knowledge, support all effort for writing books, not 8-pager conference papers...
1523789509279715328 2022-05-09 22:18:24 +0000 <YiMaTweets> @matloff We did save everything... maybe one day will be released for greater good... at least I can tell you one thing: he or she did not point out anything at all we did wrong in the paper. That also explains a very quick review later by JMLR for such a long paper...
1523788400884518912 2022-05-09 22:14:00 +0000 <YiMaTweets> @artistexyz Then what is to prevent us from having efficient and effective causal inference machines/algorithms, now, if computation is not an issue?
1523719336405266432 2022-05-09 17:39:33 +0000 <YiMaTweets> Talk concepts is cheap, computation is expensive! When you are penning down concepts like ML, KL, JS, MI, W-distances, or causation as if they are at your finger tips... make sure you know how hard (or impossible) to compute them, in high-dim spaces, for degenerate distributions.
1523717286883774464 2022-05-09 17:31:25 +0000 <YiMaTweets> @artistexyz I did not say compression is everything... but it explains almost all current practice of deep learning. As far as I know, people have not figured out how to make causation computable in high-dim. Emmanuel once tried with "knock off"... Concept is cheap, computation is expensive.
1523702156393558018 2022-05-09 16:31:17 +0000 <YiMaTweets> Prof. Shing-Tung Yau invited me to give a talk about the Rate Reduction work at Harvard Math about a year ago. It is recorded on YouTube if you are interested:  https://t.co/qPlXF5uta6 but SO MUCH has progressed since then...
1523699911874674688 2022-05-09 16:22:22 +0000 <YiMaTweets> Rate reduction gives a computable measure for Aristotle's: "The whole is greater than the sum of its parts": the difference between rate distortion for the whole (expansive/contrastive) and that for the parts (compressive/contractive), for all data, not just contrast two samples!
1523697991042838529 2022-05-09 16:14:44 +0000 <YiMaTweets> TMBK, rate reduction is the only computable distance between mixtures of (degenerate,  subspace-like) Gaussians, that miraculously has a closed-form! That is NOT the case for KL, JS, W-distances -- either ill-defined or intractable to compute - only to approximate with a big cost
1523695688726446089 2022-05-09 16:05:35 +0000 <YiMaTweets> One may view the linear discriminative representation (LDR) proposed in the ReduNet paper is a natural generalization of "one-hot" vectors for labels, replaced by subspaces of proper dimension, matching that of data distribution. This avoids neural collapse for learned features.
1523692684837199873 2022-05-09 15:53:39 +0000 <YiMaTweets> RaduNet allows to shift focus of learning from fitting the output/label to learning distribution of the input/data. So it enables us to learn a representation that is both discriminative and generative, hence in a way unifies GAN and AE in our latest CTRL:  https://t.co/a185jA2OxG
1523691177215545345 2022-05-09 15:47:40 +0000 <YiMaTweets> ReduNet paper ( https://t.co/CN7eurqo2w) made us realized that the law of parsimony (or compression) is the ruling principle for learning (probably both artificial &amp; natural), a truly computable one too, in terms of rate distortion (after a transform/DNN linearizing distribution)
1523540536451231744 2022-05-09 05:49:04 +0000 <YiMaTweets> Our new paper published on JMLR:  https://t.co/CN7eurqo2w The title "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction" says it all. The main body was rejected by an area chair of ICML'21 despite unanimous accepts by 4 reviewers! Now you be the jury
1523399410624868352 2022-05-08 20:28:17 +0000 <YiMaTweets> Following instructions is open-loop control‚Ä¶ this is closed-loop adaptive control‚Ä¶ always so much better.
1522998720207278082 2022-05-07 17:56:05 +0000 <YiMaTweets> @somefunAgba normally we cannot as some of them are continuing projects into the summer by the students
1522701979310460928 2022-05-06 22:16:56 +0000 <YiMaTweets> Great, just had two days of many wacky robotic concept from the EECS106B students... too many robots in two days...
1522666527425581058 2022-05-06 19:56:04 +0000 <YiMaTweets> Two days of final project showcase presentations for the EECS 106B Robotics course I am co-teaching:  https://t.co/zNjgtANQe9... hardest course on campus. So many spirited students and exciting projects. Some teams are obviously very ambitious, but even failures look glorious...
1522460680216977408 2022-05-06 06:18:06 +0000 <YiMaTweets> @m_a_schenk Thanks, that just made all that effort so worth it!
1522367631445032960 2022-05-06 00:08:22 +0000 <YiMaTweets> Last year, gave over 40 talks (thanks to Covid, mostly virtual):  https://t.co/mvzwWpGKWw. Less than half way through this year, already about 20 talks lined up. Feel so much more stressful and exhausting, as most require traveling... (visas, flights, hotels...) wish back to Meta.
1522339603465732096 2022-05-05 22:16:59 +0000 <YiMaTweets> For all collaborative research project, the hardest is not the project. It is the "collaborative" part.
1522084783097999360 2022-05-05 05:24:25 +0000 <YiMaTweets> Got a new idea for the title for the second version of my book "An Invitation to 3D Vision": "An Invitation to Meta Universe". Got to be a best seller...
1522058495402536961 2022-05-05 03:39:58 +0000 <YiMaTweets> @adityamwagh @akanazawa that is all you noticed from all these wonderful knowledge? :-)
1522058169236680704 2022-05-05 03:38:40 +0000 <YiMaTweets> @sthapa4 Well, even fancy learning (of visual perception) is itself a control problem:  https://t.co/a185jA2OxG period...
1522051990754275329 2022-05-05 03:14:07 +0000 <YiMaTweets> typo: Anjoo --&gt; Angjoo, professor Angjoo Kanazawa...
1522047664967196672 2022-05-05 02:56:56 +0000 <YiMaTweets> Angjoo gave a guest lecture in my course about her work on Plenoxel:  https://t.co/UWMxGWsJhH Sparsity seems to be all you need for recovering high-quality 3D radiance fields! One cannot help but notice the striking similarity with recovering 2D MRI from sparsity (Ch10 of my book)
1522046474237792257 2022-05-05 02:52:12 +0000 <YiMaTweets> @JoshVont We are not allowed to share them outside of the campus, as the university owns the copyright. But I might record many of the lectures myself and release them, when I can find time in the summer. For now, slides are all I can provide.
1522045942358151168 2022-05-05 02:50:05 +0000 <YiMaTweets> Lessons we learned from TILT, RASL, or NeRF etc.: if the (low-dim) generative models are known or drivable from first principles (physics, geometry), there is no need of learning anything from the data with any black-box DNNs. Just solve the darn (inverse) optimization problems!
1522044875847323648 2022-05-05 02:45:51 +0000 <YiMaTweets> Finished my Geometry&amp;Learning for 3D Vision course, with a full set of slides:  https://t.co/C6jA8vM9FP Ended with Lec10 connecting 3D Vision to low-dimensional (sparse, low-rank) models; and Lec11 from Anjoo just strikes this connection home: NeRF without DNN, but only sparsity!
1521954982152482816 2022-05-04 20:48:38 +0000 <YiMaTweets> @eisa_ayed visit the MBZUAI university... Eric Xing is a friend.
1521901825292521478 2022-05-04 17:17:25 +0000 <YiMaTweets> Two more weeks to go for the semester. After that, finally back to summer international traveling mode (after nearly three years): Singapore, Ottawa, Abu Dhabi, likely Hong Kong, and then London... talks after talks...
1521719993343234048 2022-05-04 05:14:53 +0000 <YiMaTweets> @allonsygamma Â∞±‰ªéËøôÊú¨‰π¶ÂºÄÂßãÂ∞±Â•Ω„ÄÇ
1521354615903584257 2022-05-03 05:03:00 +0000 <YiMaTweets> @ChongwenWang ‰π¶Â∑≤ÁªèÂ∞ΩÂèØËÉΩËá™Êàê‰ΩìÁ≥ª„ÄÇÂ∫îËØ•ÊòØÂ≠¶‰π†ÁöÑÊç∑ÂæÑÔºå‰πüÊòØÂ•Ω‰π¶ÁöÑ‰ª∑ÂÄº„ÄÇ
1521352055612391425 2022-05-03 04:52:50 +0000 <YiMaTweets> Mine is too free for download at:  https://t.co/yYOTz4F6T9 but your time of reading books is not free, even more costly if got misled‚Ä¶ So be sure you choose the right books to read.
1521210057114808320 2022-05-02 19:28:34 +0000 <YiMaTweets> Agreed, as we increasingly and some deliberately confusing technical reports with papers with original ideas‚Ä¶
1520971127173963777 2022-05-02 03:39:09 +0000 <YiMaTweets> Well, nature could do both: Langevin dynamics are a combination of both‚Ä¶ with some price, has a chance to reach global optima despite nonconvex landscapes  (chapter 9 of my new book)‚Ä¶ understanding optimization needs a systematic view, not mechanisms in isolation
1520963143899353088 2022-05-02 03:07:26 +0000 <YiMaTweets> To trust their tweets‚Ä¶
1520934638914392064 2022-05-02 01:14:10 +0000 <YiMaTweets> I wonder whether the distance defined by Maximal Coding Rate Reduction (via a nonlinear embedding for high-dim degenerate distributions) will leave a mark in history...
1520905507208572929 2022-05-01 23:18:24 +0000 <YiMaTweets> Some fans in the Bay Area bought copies of my new book and came to my house for signatures. Hope it‚Äôs helpful‚Ä¶  https://t.co/JgKd0sy084
1520889993295646720 2022-05-01 22:16:45 +0000 <YiMaTweets> Laboring on international labor day...
1520884311007789056 2022-05-01 21:54:11 +0000 <YiMaTweets> @_jaku_xu Yes, self-fulfilling... not to make others jealous...
1520807389246541825 2022-05-01 16:48:31 +0000 <YiMaTweets> Don Geman, not Don German‚Ä¶  sometimes hate autocorrection
1520806361566646274 2022-05-01 16:44:26 +0000 <YiMaTweets> In the EECS 208 website:  https://t.co/iMAz9JOIv4 l mentioned the course is a journey from L0, to L1 and to L4‚Ä¶. Well, as you age, the norm you use to measure your papers‚Äô contributions go in the same way. Eventually, L_infinity for a single most important paper or book‚Ä¶
1520804717000986629 2022-05-01 16:37:54 +0000 <YiMaTweets> In terms of measure for research contribution, l always liked Don German‚Äôs suggestion: everybody gets five token papers for lifetime, on their CV or website‚Ä¶ hide the rest.
1520802627474264064 2022-05-01 16:29:36 +0000 <YiMaTweets> @israr19548262 Remove your supervisor from the respect list‚Ä¶
1520802294123483136 2022-05-01 16:28:16 +0000 <YiMaTweets> @bcjanmay about 4 or 5 in google‚Ä¶ and others
1520796480314900482 2022-05-01 16:05:10 +0000 <YiMaTweets> Some students once asked me how to measure success in research, # of papers or citations? No! Earn respect from people you respect, never envy from people you envy‚Ä¶ the one you should respect the most is yourself‚Ä¶
1520667618046214144 2022-05-01 07:33:07 +0000 <YiMaTweets> Well, for those wonder who those people are in the photo. For one, professor Zhaojun Bai, the one on the left, is one of the leading authors of the legendary LAPACK, written in FORTRAN... forked to many modern high performance computing tools...
1520655219608080385 2022-05-01 06:43:51 +0000 <YiMaTweets> Visit UC Davis today and met my former colleagues from days at Shanghaitech‚Ä¶ professor Ding and Bai‚Ä¶ good old days ‚Ä¶  https://t.co/Y9udIfEFPA
1520309949582110720 2022-04-30 07:51:52 +0000 <YiMaTweets> @BlindDou Everything
1520305917962895360 2022-04-30 07:35:51 +0000 <YiMaTweets> Conference publications in AI related areas can surely use some serious rate reduction‚Ä¶     I heard some authors submit over 60 papers to a single conference‚Ä¶ just indicating how low the bar has been perceived by many for so called ‚Äútop‚Äù conferences‚Ä¶
1520268846573051904 2022-04-30 05:08:32 +0000 <YiMaTweets> @YanjunLi1 Actually, we are collecting all relevant examples for exercises and demos as teaching material from instructors or researchers. If you have good material (slides, examples, figures), please feel free to send to me.
1520104275547807745 2022-04-29 18:14:36 +0000 <YiMaTweets> @rezar The last 10% is about 50% of the work, if you want it to be good. I had the draft ready 4 years ago, and without the pandemic locking at home for this, I am looking for another couple of years at least.
1520086100982738944 2022-04-29 17:02:22 +0000 <YiMaTweets> There has been a lot discussions about how to reform EE &amp; CS curricula lately. The lack of top-down design or update of obsolete material is striking. If you go to a Michelin star restaurant, you expect a set menu, not a buffet... and do you expect it changes once a while.
1519910126831906817 2022-04-29 05:23:07 +0000 <YiMaTweets> I was asked in research where intuition of a fundamental idea may come from. Well, like solving a jigsaw puzzle, after you have seen enough pieces, you intuition starts to see the big picture... the key is, without the hard work with the early pieces, intuition will never come.
1519909355784523776 2022-04-29 05:20:03 +0000 <YiMaTweets> @RRKRahul96 Well, like solving a jigsaw puzzle, after you have seen enough pieces, you intuition starts to see the big picture... the key is, without the hard work with the early pieces, intuition will never come.
1519908450305601536 2022-04-29 05:16:27 +0000 <YiMaTweets> @moezodole Yes, that is true. I meant epiphany for me, for now. Well, I have written three books, and I have a sense about what I did would stand in history.
1519907974822694912 2022-04-29 05:14:34 +0000 <YiMaTweets> @QRJ211 ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction:  https://t.co/pB89L0VdD7 and subsequent works.
1519845915099422721 2022-04-29 01:07:58 +0000 <YiMaTweets> You get what you reward...
1519753728101740544 2022-04-28 19:01:39 +0000 <YiMaTweets> Perseverance is probably the most important virtue for research. All my latest research breakthroughs on (deep) learning started from work back in 2007, trying to resolve fundamental difficulties in clustering high-dim data to which few pay attention then:  https://t.co/hmAwUm9m8Z
1519752077370531841 2022-04-28 18:55:05 +0000 <YiMaTweets> I am slow to new tech, such as social networks. But my first name Chinese character "ÊØÖ‚Äù means "persistence" or "perseverance". Though I joined Chinese twitter Weibo late (only in 2013ish),  I have been using it actively for about 10 years, even my kids had asked me to move on...
1519724079170813952 2022-04-28 17:03:50 +0000 <YiMaTweets> @SergeBelongie They are always way ahead of me in terms of technology ... we are dinosaurs to them.
1519723779852619776 2022-04-28 17:02:38 +0000 <YiMaTweets> @SergeBelongie might give a try later... thanks for introducing that to me early.
1519723475090370562 2022-04-28 17:01:26 +0000 <YiMaTweets> @SergeBelongie I start to see that a little too... social networks in China are go through different kind of shaking up... too much dust...
1519722280670031873 2022-04-28 16:56:41 +0000 <YiMaTweets> geez, just joined twitter and you are talking about its downfall... should I also consider mastodon? hmmm
1519554553850462208 2022-04-28 05:50:12 +0000 <YiMaTweets> @MSadeghee By my understanding of the rules, recorded course lectures cannot be shared publicly without university's formal approval. I plan to record them myself in the future when things are more stable, including the lectures for my new book/course.
1519552056171778048 2022-04-28 05:40:16 +0000 <YiMaTweets> @d_i_j_k_stra that is why you should catch up with some reading or watching some of my recent talks... they are not that difficult to locate.
1519551812503711744 2022-04-28 05:39:18 +0000 <YiMaTweets> @d_i_j_k_stra you could start with some of our papers. we are shifting the practice and shifting it big...
1519543785675362305 2022-04-28 05:07:24 +0000 <YiMaTweets> @yelingqun @ncyanghk We have different faith then‚Ä¶
1519535329878822913 2022-04-28 04:33:48 +0000 <YiMaTweets> @ZongYuhui I am not a philosopher...
1519534900814049280 2022-04-28 04:32:06 +0000 <YiMaTweets> @roeldobbe We can start to explain the roles of deep networks and their functions from first principles (with principles form control and info. theory), although there are still many open problems.
1519534420692070400 2022-04-28 04:30:12 +0000 <YiMaTweets> @roeldobbe Only if you/we believe they are behind intelligence -- as intelligence needs to be efficiently implementable.
1519533955829370880 2022-04-28 04:28:21 +0000 <YiMaTweets> @gdsttian sometimes, optimality of mathematical principles do not need experiments to justify.
1519533312104042496 2022-04-28 04:25:47 +0000 <YiMaTweets> @ncyanghk animals are diverse, but the rules behind life and evolution are unifying...
1519419812723642368 2022-04-27 20:54:47 +0000 <YiMaTweets> @d_i_j_k_stra that is where mathematics comes in to shed light on what is ultimately optimal.
1519416745529724928 2022-04-27 20:42:36 +0000 <YiMaTweets> I am developing and teaching a new course on 3D Vision at Berkeley this semester: CS294: Geometry and Learning for 3D Vision. It will be finished in about two weeks. I will make all the lecture slides available. It is time to close the loop between reconstruction and recognition.
1519412212095324160 2022-04-27 20:24:35 +0000 <YiMaTweets> Regarding 3D reconstruction, in the past 2-3 years, some of my former students of UIUC have initiated an effort to integrate geometry and learning, starting with a tutorial at ICCV'19  https://t.co/iw5yQstsBB and a few subsequent workshops at CVPR'21, ICCV'21, and upcoming ECCV'22
1519410648160997376 2022-04-27 20:18:22 +0000 <YiMaTweets> Thanks for reminding me of this book. It was published just about 20 years ahead of its time -- most of the geometric principles laid out in it have not nearly been exploited by any modern 3D reconstruction systems, SfM or SLAMs barely scratching the surface for the information.
1519409460527046660 2022-04-27 20:13:39 +0000 <YiMaTweets> @west_of_time123 yes, pity and thought provoking...
1519378454713565185 2022-04-27 18:10:26 +0000 <YiMaTweets> @ak_panda my recent talks and papers, from my personal websites.
1519377966853107713 2022-04-27 18:08:30 +0000 <YiMaTweets> Had a chat with LeCun yesterday. Discussed that while "divide and conquer" is a cherished tenet for Science, for understanding natural or artificial intelligence, we need to "compound and conquer", precisely by integrating pieces from all the years of "blind men and an elephant".
1519376832889430016 2022-04-27 18:04:00 +0000 <YiMaTweets> Enough with all the lego games with network architectures. We have a much more significant scientific jigsaw puzzle to put together what (artificial or natural) learning is all about, with pieces from control, information/coding theory, optimization, and machine learning...
1519369858349027329 2022-04-27 17:36:17 +0000 <YiMaTweets> Who can explain all these "celebrated" papers claiming MLP&lt;CNN&lt;ViT&lt;CNN&lt;MLP... Aren't people having better things to do? We need a theory and a framework that unifies all these instances, not fighting over trivia...
1519219446883098625 2022-04-27 07:38:36 +0000 <YiMaTweets> Intelligence is measured by "Intelligence Quotient" (IQ). As a quotient, it must has a numerator and denominator. So, for AI, where should the model size and computational resource go?
1519187471078072320 2022-04-27 05:31:32 +0000 <YiMaTweets> My reply: Indeed, principles from control and information theory are at the heart of any learning, as long as they are made computable and scalable. I believe we are starting to enter a different phase of machine learning. It is the beginning of the end of the empirical phase...
1519186895774773249 2022-04-27 05:29:15 +0000 <YiMaTweets> A researcher at FAIR, previously at GoogleX (who tool my linear system course many years ago at UIUC) contacted me after hearing my talk at FAIR. He found our new closed-loop control approach to ML "refreshing and exceptionally promising".
1519014256552800256 2022-04-26 18:03:15 +0000 <YiMaTweets> @Jvvk80 It is in the preface... college level linear algebra, probability, optimization...
1519013614057050112 2022-04-26 18:00:42 +0000 <YiMaTweets> Please feel free to retweet or inform anyone who cannot afford the book, about the preproduction version... but for those who can, support Cambridge University Press, they did a marvelous job copyediting...
1519012872914825221 2022-04-26 17:57:45 +0000 <YiMaTweets> I asked Stephen Boyd to recommended his book editor so that we can get the same deal to release preproduction version of our book online, free for everyone:  https://t.co/4XEBJXNwqn (on Github, dropbox, CSDN), supported with a full deck of lecture slides:  https://t.co/iMAz9JOIv4.
1518640888049639424 2022-04-25 17:19:37 +0000 <YiMaTweets> @yintianchou I do not discuss politics here.
1518630543092559872 2022-04-25 16:38:30 +0000 <YiMaTweets> One month ago, my former students encouraged me to start a twitter account. One of the main reasons is that my Chinese "twitter" Weibo account might get suspended if I say anything sensitive (to someones). Never imagine to have over 5,000 followers in one month. Truly humbled...
1518345418840547328 2022-04-24 21:45:32 +0000 <YiMaTweets> @adjiboussodieng I know. But that is not what we are getting though, aren‚Äôt we? Discriminative or generative models‚Ä¶ our LDR is one attempt.
1518304084205309952 2022-04-24 19:01:17 +0000 <YiMaTweets> If we do not strive to understand the very internal mechanisms of networks, even after all these trial-and-error, we might found the most effective networks are human brains. We might as well produce babies on an industrial scale, and train them. The movie Matrix suggested it...
1518302910269984769 2022-04-24 18:56:37 +0000 <YiMaTweets> I found the very name of "hidden" or "latent" representations very pessimistic and disappointing: after spending millions and billions of dollars gathering data and training big models, what you get is hidden. Shouldn't we get something transparent, compact, and highly organized?
1518301531673489408 2022-04-24 18:51:08 +0000 <YiMaTweets> Since that conversation, the goal of my group has always been making learning pure white-boxes, including: 1. making the objective of (deep) learning a white box: 2. making the architectures and operators of networks a white box; 3. making the representations learned a white box!
1518300194235846656 2022-04-24 18:45:49 +0000 <YiMaTweets> Going to give a talk tomorrow morning on "CTRL: Closed-Loop Data Transcription via Rate Reduction" at FAIR/MAIR, to LeCun's group. Actually, my whole line of research on Rate Reduction started with a conversation with Yann in late 2019: what are deep neural networks optimizing?
1517945457762201600 2022-04-23 19:16:13 +0000 <YiMaTweets> In a recent conversation with my editor, Julie, of CUP, we both believe high-standard books will be increasingly important for the society, in which information, even knowledge, comes increasingly fragmented, as fast food. International Day of Reading: read books, not tweets!
1517932247478796288 2022-04-23 18:23:44 +0000 <YiMaTweets> @rezar Of course not! What I said is on behalf of all those who have been fighting against the hype. But we were one of  a few groups taking serious actions. You could tell from my new book how to exercise a complete different level of standards for giving credit to all that is due.
1517930101500833792 2022-04-23 18:15:12 +0000 <YiMaTweets> Today is the International Day of Reading. What else can an author of a new book recommend:   https://t.co/Rd0aHtGNvu
1517911467483299840 2022-04-23 17:01:09 +0000 <YiMaTweets> The survey l did about optimal control and reinforcement learning draws ideas from these lectures:  https://t.co/G5ndMUZKie
1517727209611350016 2022-04-23 04:48:59 +0000 <YiMaTweets> @RogerFrigola Our latest framework based rate reduction and closed-loop system meant to make that principled and systematic.
1517726684098621441 2022-04-23 04:46:54 +0000 <YiMaTweets> @dongtianbox you could take a look at our CTRL work:  https://t.co/a185jA2OxG meant to simplify things, a LOT.
1517576452178206720 2022-04-22 18:49:56 +0000 <YiMaTweets> @tz33cu yes, sciences are in the danger of being reduced to the standard of pop music or posters....
1517553160314769409 2022-04-22 17:17:22 +0000 <YiMaTweets> Along the years, that simple baseline (purely forward constructed) PCAnet has become the ancestor for the (also forwarded constructed) ReduNet. Last sentence of my new book, we left the reader with: "We compress to learn, and we learn to compress!" Intelligence is parsimonious.
1517551390251372545 2022-04-22 17:10:20 +0000 <YiMaTweets> About Small for learning, my first paper about deep learning is "PCAnet: a simple baseline for deep learning":  https://t.co/3kkeN0EgfX  A SOTA two-layer network without BP! Imagine the drama trying to publish that back in 2014, rejected by all CV/DL conferences, but made to TPAMI
1517549973264502784 2022-04-22 17:04:42 +0000 <YiMaTweets> Andrew Ng on new IEEE Spectrum cover: "In AI, Small is the New Big." Very baffled: isn't that what I have been advocating against all odds in the past ten years, while Andrew and others are advocating the opposite? Maybe tomorrow "closed-loop is in now". Science is NOT fashion.
1517365404473077761 2022-04-22 04:51:18 +0000 <YiMaTweets> @pappasg69 @Stanford @CSSIEEE @ieeeras @NeurIPSConf Control for Learning is a more appropriate name.
1517215862045876224 2022-04-21 18:57:04 +0000 <YiMaTweets> Or we could also say: Everything intelligent in the Universe is a closed-loop system.
1517207846894465024 2022-04-21 18:25:13 +0000 <YiMaTweets> So far, our work has discovered two (both quantifiable and computable) principles that govern intelligence (natural or artificial): Parsimony and Self-Consistency, which already explains almost everything we know of. I have not found a third.
1517205848241496065 2022-04-21 18:17:17 +0000 <YiMaTweets> Everything alive in the universe is a closed-loop system!
1517205058445742080 2022-04-21 18:14:08 +0000 <YiMaTweets> Regarding the relationships between control and learning, I did a thorough survey and comparison between Optimal Control and Reinforcement Learning last year for a course, with slides:  https://t.co/hvwAS4Y2eq and a youtube recording:  https://t.co/A3tHUXaXEr Similarity is uncanny.
1517044769049915392 2022-04-21 07:37:12 +0000 <YiMaTweets> Yes, l have been keeping telling my control colleagues they need to make their principles computable for high-dim and large scale problems‚Ä¶
1517038343032229889 2022-04-21 07:11:40 +0000 <YiMaTweets> According to the three goals of AI by Yann, AI is increasingly becoming the three goals of autonomous control: system identification, decision making, and control...
1516882421261606913 2022-04-20 20:52:06 +0000 <YiMaTweets> About a year ago today, I gave a talk on Deep Networks from First Principles at Harvard Mathematics:  https://t.co/qPlXF5uta6 What a year has it been and how much progress have we made since then! The full ReduNet paper is also about to appear in JMLR soon, after a lot of drama.
1516149820934680576 2022-04-18 20:21:00 +0000 <YiMaTweets> I am a Plenary Speaker at the 2022 IEEE International Conference on Acoustics, Speech &amp; Signal Processing (@ieeeICASSP). Join me, 22-27 May 2022!   https://t.co/hhfFUPx9fE
1513959490546139136 2022-04-12 19:17:25 +0000 <YiMaTweets> We're asked to produce a figure of our CTRL paper:  https://t.co/a185jA2OxG for the upcoming issue of Entropy, as a perfect unity of control and information theory. The paper was rejected by ICLR'21 but now likely goes to the cover of a magazine.  https://t.co/LifOgWhYDj
1513327118880411651 2022-04-11 01:24:36 +0000 <YiMaTweets> The publisher is having a promotion for our new book till the end of Dec 2022. The 20% discount code is ‚ÄúHDDA2022‚Äù. It is for global use except in countries where direct sales is restricted. See the attached flyers.  https://t.co/mINqpAFTXT
1512464511114096645 2022-04-08 16:16:54 +0000 <YiMaTweets> Going to visit UW today and giving a seminar at IFDS of ECE at 1:30pm... Hope the Cherry Blossom is still good at UW.
1512103754656452617 2022-04-07 16:23:23 +0000 <YiMaTweets> @DrorSimon RNN is just identifying the system. It is still open loop. Back propagation is "feedback" but not closed-loop the system. Actually RL has a flavor of closed-loop. But not as systematic as should.
1511965091117248512 2022-04-07 07:12:23 +0000 <YiMaTweets> Nature seems to be a master of close-loop transcription that naturally realizes the principles of Parsimony and Self-consistency: our motor control, our memory forming, and transcription btw proteins and the DNA. Enough open-loop end-to-end networks, it's time to close the loops!
1511963330469978113 2022-04-07 07:05:23 +0000 <YiMaTweets> Our recent studies seem to indicate probably only two  principles govern learning: Parsimony and Self-consistency. For parsimony, one maps the data on to a simple structured model (say LDR); for self-consistency, one can reconstruct the data from the model up to a given accuracy.
1511961489300615168 2022-04-07 06:58:04 +0000 <YiMaTweets> About learning, I always believe "we learn to compress and we compress to learn," regardless of  supervised or unsupervised settings, or discriminative or generative purposes. A long line of my work aims to make this belief computable. Nature follows the same Law of Parsimony...  https://t.co/JkBncp9Itu
1511755572470640643 2022-04-06 17:19:50 +0000 <YiMaTweets> @ghoshd you will thank me later.ü§≠
1511451942299439108 2022-04-05 21:13:19 +0000 <YiMaTweets> Incidental trilogy: "High-Dimensional Probability" by Roman Vershynin; "High-Dimensional Statistics" by Martin Wainwright; and ours "High-Dimensional Data Analysis" by John Wright and me. All by the same publisher, even share the same color scheme...  https://t.co/XlbaiUDPgy
1511404057956544512 2022-04-05 18:03:02 +0000 <YiMaTweets>  https://t.co/r9W5rCD7gl the information technology is under going a revolution, so should be the eecs curricula... my new book was part of that purpose. just visited UM two weeks ago.
1511086662243151872 2022-04-04 21:01:49 +0000 <YiMaTweets> A paper from recently graduated students:  https://t.co/odwLlLfi1k Two key messages: 1. the rate reduction objective can be computed more efficiently via its Variational Form; 2. the variation form  can be naturally interpreted as Dictionary Learning. All things seem connected...
1511084268247662598 2022-04-04 20:52:18 +0000 <YiMaTweets> In Seattle this week. In about 10 years, Bellevue has transformed from a small town to a modern city... now with a new transit train running to Seattle.
1510106721384951809 2022-04-02 04:07:53 +0000 <YiMaTweets> @DaktirNikola yes. seeking low-dimensionality is universal for any data processing tasks. that is the intro of my book.
1509981187820843010 2022-04-01 19:49:03 +0000 <YiMaTweets> The only message you can believe today is that today is April fool's day...
1509794639577944068 2022-04-01 07:27:47 +0000 <YiMaTweets> @sahilsingla47 my previous retweet of Jeff Dean's
1509790934631809024 2022-04-01 07:13:04 +0000 <YiMaTweets> Deep networks are learning low-dim structures in the data and represent them as sparse models, period. The latest work from Google, is a great advertisement for our new book -- a timely arrival.
1509609056339562511 2022-03-31 19:10:20 +0000 <YiMaTweets> Shared an interesting note with my students and colleagues on the connection between sparsity and deep networks:  https://t.co/HcxsDILuoH After all, many recent works show that even big models like BERT or GPT-X are implicitly learning low-dim or sparse models for sub tasks.
1509607563515465764 2022-03-31 19:04:24 +0000 <YiMaTweets> We are organizing a short course at ICASSP'2022 on Low-dimensional models and deep networks, details at:  https://t.co/grBkeBhxCB I also plan to give a Plenary Talk after the conference's opening ceremony.
1509605039072980993 2022-03-31 18:54:23 +0000 <YiMaTweets> It seems that our new book is drawing some attention. I believe it will be a treasure hunt for most sincere readers.  https://t.co/FBIB6bfi7Q
1509560080638746626 2022-03-31 15:55:44 +0000 <YiMaTweets> @wangtnetlab because it exploits problem structures
1509397543356035075 2022-03-31 05:09:52 +0000 <YiMaTweets> @robinc check my talk website:  https://t.co/ACOPldNIpr
1509396961232764931 2022-03-31 05:07:33 +0000 <YiMaTweets> @leonpalafox  https://t.co/4XEBJXNwqn
1509352474389323776 2022-03-31 02:10:46 +0000 <YiMaTweets> Folks asked for relevant talks. I have given over 40 talks last year and about 10 this year, thanks to virtual seminars! Some keys talks are recorded, you may find links at:  https://t.co/ACOPldNIpr the most popular is "Deep Networks from First Principles", given to Harvard Math.
1509347117453651968 2022-03-31 01:49:29 +0000 <YiMaTweets> Based on the new book, we have developed a new graduate course on data science: Berkeley EECS 208 "Computational Principles for High-dimensional Data Analysis":  https://t.co/PxHhjHR0Wd, with a complete set of lecture notes and supplementary materials at:  https://t.co/iMAz9JOIv4.
1509271783983108096 2022-03-30 20:50:08 +0000 <YiMaTweets> Finally, my new book on "High-Dim Data Analysis with Low-Dim Models: Principles, Computation, and Applications" is available in stock on Amazon:  https://t.co/Rd0aHtGNvu for anyone who needs a rigorous mathematical &amp;  computational foundation in data science and machine learning.  https://t.co/bFJHFQvNU4
1507559176116998144 2022-03-26 03:24:51 +0000 <YiMaTweets> Our latest work shows an arguably much simpler and more principled way than GANs or VAEs for learning generative and discriminative models altogether, called Closed-Loop Transcription, kind of emulating the universal mechanism of transcription in Nature:  https://t.co/a185jA2OxG
1507538081401827331 2022-03-26 02:01:01 +0000 <YiMaTweets> Recommended by one of my former students, Dr. Hossein Mobahi, I should start using a twitter account. Here we go.
[!] No more data! Scraping will stop now.
found 0 deleted tweets in this search.
['twitter'] poster drawer done in `OUT_FOLDER`
